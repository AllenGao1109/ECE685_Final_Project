{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "147d36d3-f569-4f31-bd71-311c8332d83f",
   "metadata": {},
   "source": [
    "## Group Project LLM\n",
    "\n",
    "- r=2,4,8,16, epoch=30\n",
    "- seed=42\n",
    "- evaluation:\n",
    "    - accuracy, f1, precision, recall\n",
    "    - efficiency (time, trainable parameters, trainable paramters ratio, early stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9402eeb1-9507-4fd7-98ee-f5e80a71de37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"default\", module=\"__main__\")\n",
    "warnings.filterwarnings(\"ignore\", module=\".*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92de0fe4-12ab-49a1-82bf-76d233877bf9",
   "metadata": {},
   "source": [
    "### Base Model: DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9beb58e-f6ab-4a7f-b213-61f66d3dcc33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPU Info] 1 GPU(s) available\n",
      "  GPU 0: NVIDIA RTX 5000 Ada Generation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nmodel = DistilBertForSequenceClassification.from_pretrained(\\n    \"distilbert-base-uncased\", num_labels=2\\n).to(DEVICE)\\n\\ntotal_params = sum(p.numel() for p in model.parameters())\\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\\nratio = trainable_params / total_params\\n\\nprint(f\"Baseline: total={total_params}, trainable={trainable_params}, ratio={ratio:.4%}\")\\n\\n# -------- Train --------\\nargs = TrainingArguments(\\n    output_dir=\"./baseline_distilbert\",\\n    num_train_epochs=NUM_EPOCHS,\\n    per_device_train_batch_size=BATCH_SIZE,\\n    per_device_eval_batch_size=BATCH_SIZE,\\n    learning_rate=LR,\\n    eval_strategy=\"epoch\",\\n    save_strategy=\"epoch\",\\n    logging_strategy=\"epoch\",\\n    load_best_model_at_end=True,\\n    metric_for_best_model=\"f1\",\\n    greater_is_better=True,\\n    seed=SEED,\\n    report_to=\"none\",\\n)\\n\\ntrainer = Trainer(\\n    model=model,\\n    args=args,\\n    train_dataset=train_ds,\\n    eval_dataset=val_ds,\\n    data_collator=collator,\\n    tokenizer=tokenizer,\\n    compute_metrics=compute_metrics,\\n)\\n\\nstart = time.time()\\ntrainer.train()\\nend = time.time()\\n\\nprint(f\"Baseline training time: {end-start:.2f}s\")\\nprint(\"Eval:\", trainer.evaluate(test_ds))\\nprint(\"Training history:\", trainer.state.log_history)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ================== BASELINE DISTILBERT ================\n",
    "# Commented out - not needed for Sparse LoRA experiments\n",
    "\n",
    "import os, time, random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    DistilBertTokenizerFast,\n",
    "    DistilBertForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    set_seed\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "# Force to use only GPU 1\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# Print GPU info\n",
    "if torch.cuda.is_available():\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"[GPU Info] {num_gpus} GPU(s) available\")\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"[GPU Info] No GPU available, using CPU\")\n",
    "\n",
    "SEED = 42\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DATASET = \"stanfordnlp/sst2\"\n",
    "TEXT_COL = \"sentence\"\n",
    "LABEL_COL = \"label\"\n",
    "NUM_EPOCHS = 30\n",
    "BATCH_SIZE = 16\n",
    "LR = 5e-4\n",
    "\n",
    "'''\n",
    "for IMDB dataset:\n",
    "DATASET = \"imdb\"\n",
    "TEXT_COL = \"text\"\n",
    "LABEL_COL = \"label\"\n",
    "'''\n",
    "\n",
    "def set_all_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    set_seed(seed)\n",
    "\n",
    "set_all_seeds(SEED)\n",
    "\n",
    "# -------- Load dataset and split (8:1:1) --------\n",
    "raw = load_dataset(DATASET)\n",
    "train_full = raw[\"train\"]\n",
    "\n",
    "train_temp = train_full.train_test_split(test_size=0.2, seed=SEED)\n",
    "train_ds = train_temp[\"train\"]\n",
    "temp = train_temp[\"test\"]\n",
    "\n",
    "val_test = temp.train_test_split(test_size=0.5, seed=SEED)\n",
    "val_ds = val_test[\"train\"]\n",
    "test_ds = val_test[\"test\"]\n",
    "\n",
    "\n",
    "# -------- Tokenization --------\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "def preprocess(x):\n",
    "    return tokenizer(x[TEXT_COL], truncation=True, max_length=128)\n",
    "\n",
    "train_ds = train_ds.map(preprocess, batched=True)\n",
    "val_ds   = val_ds.map(preprocess, batched=True)\n",
    "test_ds  = test_ds.map(preprocess, batched=True)\n",
    "\n",
    "train_ds = train_ds.rename_column(LABEL_COL, \"labels\")\n",
    "val_ds   = val_ds.rename_column(LABEL_COL, \"labels\")\n",
    "test_ds  = test_ds.rename_column(LABEL_COL, \"labels\")\n",
    "\n",
    "cols = [\"input_ids\",\"attention_mask\",\"labels\"]\n",
    "train_ds.set_format(type=\"torch\", columns=cols)\n",
    "val_ds.set_format(type=\"torch\", columns=cols)\n",
    "test_ds.set_format(type=\"torch\", columns=cols)\n",
    "\n",
    "collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# -------- Metrics --------\n",
    "acc = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "prec = evaluate.load(\"precision\")\n",
    "rec = evaluate.load(\"recall\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": acc.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "        \"f1\": f1.compute(predictions=preds, references=labels, average=\"binary\")[\"f1\"],\n",
    "        \"precision\": prec.compute(predictions=preds, references=labels, average=\"binary\")[\"precision\"],\n",
    "        \"recall\": rec.compute(predictions=preds, references=labels, average=\"binary\")[\"recall\"],\n",
    "    }\n",
    "\n",
    "# -------- Model --------\n",
    "# COMMENTED OUT: Baseline full fine-tuning (not needed for Sparse LoRA)\n",
    "'''\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2\n",
    ").to(DEVICE)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "ratio = trainable_params / total_params\n",
    "\n",
    "print(f\"Baseline: total={total_params}, trainable={trainable_params}, ratio={ratio:.4%}\")\n",
    "\n",
    "# -------- Train --------\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./baseline_distilbert\",\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LR,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    seed=SEED,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "trainer.train()\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Baseline training time: {end-start:.2f}s\")\n",
    "print(\"Eval:\", trainer.evaluate(test_ds))\n",
    "print(\"Training history:\", trainer.state.log_history)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b955f22-3fde-4792-a715-06f14bc6ee90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport pandas as pd\\n# -------- Final Evaluation --------\\nfinal_metrics = trainer.evaluate(test_ds)\\n\\n# -------- Save metrics --------\\nos.makedirs(\"./baseline_distilbert\", exist_ok=True)\\nwith open(\"./baseline_distilbert/final_metrics.json\", \"w\") as f:\\n    json.dump(final_metrics, f, indent=4)\\n\\nprint(\"Saved final metrics to baseline_distilbert/final_metrics.json\")\\n\\n# -------- Save model --------\\ntrainer.save_model(\"./baseline_distilbert/final_model\")\\nprint(\"Saved model to baseline_distilbert/final_model\")\\n\\n# -------- Training history --------\\nlog_history = trainer.state.log_history\\ndf_logs = pd.DataFrame(trainer.state.log_history)\\n# Separate clean tables\\ndf_train = df_logs[df_logs[\"loss\"].notnull()].reset_index(drop=True)\\ndf_eval  = df_logs[df_logs[\"eval_loss\"].notnull()].reset_index(drop=True)\\n\\ndf_train.to_csv(\"./baseline_distilbert/train_log.csv\", index=False)\\ndf_eval.to_csv(\"./baseline_distilbert/eval_log.csv\", index=False)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# COMMENTED OUT: Baseline model saving and evaluation\n",
    "'''\n",
    "import pandas as pd\n",
    "# -------- Final Evaluation --------\n",
    "final_metrics = trainer.evaluate(test_ds)\n",
    "\n",
    "# -------- Save metrics --------\n",
    "os.makedirs(\"./baseline_distilbert\", exist_ok=True)\n",
    "with open(\"./baseline_distilbert/final_metrics.json\", \"w\") as f:\n",
    "    json.dump(final_metrics, f, indent=4)\n",
    "\n",
    "print(\"Saved final metrics to baseline_distilbert/final_metrics.json\")\n",
    "\n",
    "# -------- Save model --------\n",
    "trainer.save_model(\"./baseline_distilbert/final_model\")\n",
    "print(\"Saved model to baseline_distilbert/final_model\")\n",
    "\n",
    "# -------- Training history --------\n",
    "log_history = trainer.state.log_history\n",
    "df_logs = pd.DataFrame(trainer.state.log_history)\n",
    "# Separate clean tables\n",
    "df_train = df_logs[df_logs[\"loss\"].notnull()].reset_index(drop=True)\n",
    "df_eval  = df_logs[df_logs[\"eval_loss\"].notnull()].reset_index(drop=True)\n",
    "\n",
    "df_train.to_csv(\"./baseline_distilbert/train_log.csv\", index=False)\n",
    "df_eval.to_csv(\"./baseline_distilbert/eval_log.csv\", index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19b5f110-1f56-40f2-9b8a-d72926786bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport matplotlib.pyplot as plt\\n\\ndf_eval_clean = df_eval.groupby(\"epoch\").last().reset_index()\\n\\nplt.figure(figsize=(8,5))\\nplt.plot(df_train[\"epoch\"], df_train[\"loss\"], label=\"Train Loss\")\\nplt.plot(df_eval_clean[\"epoch\"], df_eval_clean[\"eval_loss\"], label=\"Eval Loss\")\\nplt.xlabel(\"Epoch\")\\nplt.ylabel(\"Loss\")\\nplt.title(\"Training: Train vs Eval Loss\")\\nplt.legend()\\nplt.grid()\\nplt.savefig(\"./baseline_distilbert/loss_curve.png\", dpi=300)\\nplt.show()\\n\\n\\nplt.figure(figsize=(8,5))\\nplt.plot(df_eval_clean[\"epoch\"], df_eval_clean[\"eval_accuracy\"], label=\"Eval Accuracy\")\\nplt.plot(df_eval_clean[\"epoch\"], df_eval_clean[\"eval_f1\"], label=\"Eval F1\")\\nplt.xlabel(\"Epoch\")\\nplt.ylabel(\"Metric\")\\nplt.title(\"Evaluation: Accuracy & F1\")\\nplt.legend()\\nplt.grid()\\nplt.savefig(\"./baseline_distilbert/metric_curve.png\", dpi=300)\\nplt.show()\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# COMMENTED OUT: Baseline visualization\n",
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_eval_clean = df_eval.groupby(\"epoch\").last().reset_index()\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(df_train[\"epoch\"], df_train[\"loss\"], label=\"Train Loss\")\n",
    "plt.plot(df_eval_clean[\"epoch\"], df_eval_clean[\"eval_loss\"], label=\"Eval Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training: Train vs Eval Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(\"./baseline_distilbert/loss_curve.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(df_eval_clean[\"epoch\"], df_eval_clean[\"eval_accuracy\"], label=\"Eval Accuracy\")\n",
    "plt.plot(df_eval_clean[\"epoch\"], df_eval_clean[\"eval_f1\"], label=\"Eval F1\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Metric\")\n",
    "plt.title(\"Evaluation: Accuracy & F1\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(\"./baseline_distilbert/metric_curve.png\", dpi=300)\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e3c60d-a93c-4673-8988-ff0c91314b0c",
   "metadata": {},
   "source": [
    "### Sparse LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b148015-5dab-49fd-af46-4a0061408da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Training Sparse LoRA DistilBERT with rank = 2, epochs = 30\n",
      "================================================================================\n",
      "[Rank 2] total params: 67,620,868\n",
      "[Rank 2] trainable params: 665,858\n",
      "[Rank 2] trainable params ratio (trainable / total): 0.9847%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='37048' max='101040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 37048/101040 27:06 < 46:49, 22.77 it/s, Epoch 11/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.278700</td>\n",
       "      <td>0.222086</td>\n",
       "      <td>0.922197</td>\n",
       "      <td>0.927584</td>\n",
       "      <td>0.928354</td>\n",
       "      <td>0.926816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.232400</td>\n",
       "      <td>0.210075</td>\n",
       "      <td>0.927988</td>\n",
       "      <td>0.931815</td>\n",
       "      <td>0.949026</td>\n",
       "      <td>0.915217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.241300</td>\n",
       "      <td>0.198828</td>\n",
       "      <td>0.936154</td>\n",
       "      <td>0.940410</td>\n",
       "      <td>0.943811</td>\n",
       "      <td>0.937034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.180500</td>\n",
       "      <td>0.235969</td>\n",
       "      <td>0.936897</td>\n",
       "      <td>0.941565</td>\n",
       "      <td>0.937568</td>\n",
       "      <td>0.945595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.184200</td>\n",
       "      <td>0.237557</td>\n",
       "      <td>0.938976</td>\n",
       "      <td>0.942348</td>\n",
       "      <td>0.957526</td>\n",
       "      <td>0.927644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.206200</td>\n",
       "      <td>0.207979</td>\n",
       "      <td>0.939569</td>\n",
       "      <td>0.944070</td>\n",
       "      <td>0.939551</td>\n",
       "      <td>0.948633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.166100</td>\n",
       "      <td>0.225069</td>\n",
       "      <td>0.940906</td>\n",
       "      <td>0.945134</td>\n",
       "      <td>0.943573</td>\n",
       "      <td>0.946700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.169300</td>\n",
       "      <td>0.238490</td>\n",
       "      <td>0.942242</td>\n",
       "      <td>0.946085</td>\n",
       "      <td>0.949638</td>\n",
       "      <td>0.942557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.156800</td>\n",
       "      <td>0.241769</td>\n",
       "      <td>0.939866</td>\n",
       "      <td>0.944161</td>\n",
       "      <td>0.942731</td>\n",
       "      <td>0.945595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.154100</td>\n",
       "      <td>0.255681</td>\n",
       "      <td>0.942242</td>\n",
       "      <td>0.945511</td>\n",
       "      <td>0.959352</td>\n",
       "      <td>0.932063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.152000</td>\n",
       "      <td>0.235393</td>\n",
       "      <td>0.940460</td>\n",
       "      <td>0.944636</td>\n",
       "      <td>0.944506</td>\n",
       "      <td>0.944767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Early Stopping] No improvement over best F1 for 3 consecutive epochs. Stopping training at epoch 11.\n",
      "[Early Stopping Info] Best F1: 0.9461 at epoch 8\n",
      "\n",
      "\n",
      "[GPU Info] Used 1 GPU(s), Current device: 0 (NVIDIA RTX 5000 Ada Generation)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='842' max='421' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [421/421 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Results for rank = 2:\n",
      "  Test Accuracy: 0.9424\n",
      "  Test F1: 0.9487\n",
      "  Validation Accuracy: 0.9422\n",
      "  Validation F1: 0.9461\n",
      "  Total Training Time: 1627.23s\n",
      "  Average Epoch Time: 142.58s\n",
      "  Best F1 Epoch: 8\n",
      "  Best Val F1: 0.9461\n",
      "  Early Stopped: True\n",
      "  Stop Info: epochs_without_improvement=3\n",
      "  Peak GPU Memory: 590.62 MB\n",
      "  LoRA Sparsity (<1e-3): 2.86%\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rank 2] Saved model to /hpc/group/xielab/hl385/LoRA/sparse_lora_rank2/final_model\n",
      "[Rank 2] Saved log history to log_history.json\n",
      "\n",
      "================================================================================\n",
      "Training Sparse LoRA DistilBERT with rank = 4, epochs = 30\n",
      "================================================================================\n",
      "[Rank 4] total params: 67,694,596\n",
      "[Rank 4] trainable params: 739,586\n",
      "[Rank 4] trainable params ratio (trainable / total): 1.0925%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='77464' max='101040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 77464/101040 56:13 < 17:06, 22.96 it/s, Epoch 23/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.288000</td>\n",
       "      <td>0.230818</td>\n",
       "      <td>0.922791</td>\n",
       "      <td>0.927415</td>\n",
       "      <td>0.937623</td>\n",
       "      <td>0.917426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.243500</td>\n",
       "      <td>0.228154</td>\n",
       "      <td>0.926503</td>\n",
       "      <td>0.930292</td>\n",
       "      <td>0.949138</td>\n",
       "      <td>0.912179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.246800</td>\n",
       "      <td>0.213829</td>\n",
       "      <td>0.939718</td>\n",
       "      <td>0.944185</td>\n",
       "      <td>0.940049</td>\n",
       "      <td>0.948357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.189800</td>\n",
       "      <td>0.237381</td>\n",
       "      <td>0.937045</td>\n",
       "      <td>0.941678</td>\n",
       "      <td>0.938065</td>\n",
       "      <td>0.945319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.189000</td>\n",
       "      <td>0.243215</td>\n",
       "      <td>0.938233</td>\n",
       "      <td>0.941818</td>\n",
       "      <td>0.954095</td>\n",
       "      <td>0.929854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.206800</td>\n",
       "      <td>0.217012</td>\n",
       "      <td>0.942094</td>\n",
       "      <td>0.945953</td>\n",
       "      <td>0.949374</td>\n",
       "      <td>0.942557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.191300</td>\n",
       "      <td>0.242401</td>\n",
       "      <td>0.939866</td>\n",
       "      <td>0.943190</td>\n",
       "      <td>0.958381</td>\n",
       "      <td>0.928473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.167500</td>\n",
       "      <td>0.268898</td>\n",
       "      <td>0.939569</td>\n",
       "      <td>0.944482</td>\n",
       "      <td>0.933154</td>\n",
       "      <td>0.956089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.167500</td>\n",
       "      <td>0.269143</td>\n",
       "      <td>0.941797</td>\n",
       "      <td>0.945991</td>\n",
       "      <td>0.943910</td>\n",
       "      <td>0.948081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.157200</td>\n",
       "      <td>0.264496</td>\n",
       "      <td>0.942094</td>\n",
       "      <td>0.945500</td>\n",
       "      <td>0.957001</td>\n",
       "      <td>0.934272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.156800</td>\n",
       "      <td>0.256736</td>\n",
       "      <td>0.942836</td>\n",
       "      <td>0.946580</td>\n",
       "      <td>0.951199</td>\n",
       "      <td>0.942005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.138800</td>\n",
       "      <td>0.289581</td>\n",
       "      <td>0.935561</td>\n",
       "      <td>0.940952</td>\n",
       "      <td>0.927326</td>\n",
       "      <td>0.954985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.147700</td>\n",
       "      <td>0.272600</td>\n",
       "      <td>0.943578</td>\n",
       "      <td>0.947046</td>\n",
       "      <td>0.955837</td>\n",
       "      <td>0.938415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.140300</td>\n",
       "      <td>0.301547</td>\n",
       "      <td>0.941500</td>\n",
       "      <td>0.946072</td>\n",
       "      <td>0.937856</td>\n",
       "      <td>0.954432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.149100</td>\n",
       "      <td>0.288636</td>\n",
       "      <td>0.944024</td>\n",
       "      <td>0.947978</td>\n",
       "      <td>0.947325</td>\n",
       "      <td>0.948633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.130200</td>\n",
       "      <td>0.310863</td>\n",
       "      <td>0.943281</td>\n",
       "      <td>0.947426</td>\n",
       "      <td>0.944307</td>\n",
       "      <td>0.950566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.132700</td>\n",
       "      <td>0.315923</td>\n",
       "      <td>0.941797</td>\n",
       "      <td>0.945781</td>\n",
       "      <td>0.947354</td>\n",
       "      <td>0.944214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.119200</td>\n",
       "      <td>0.327279</td>\n",
       "      <td>0.945954</td>\n",
       "      <td>0.949571</td>\n",
       "      <td>0.952738</td>\n",
       "      <td>0.946424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.123000</td>\n",
       "      <td>0.327321</td>\n",
       "      <td>0.945509</td>\n",
       "      <td>0.949077</td>\n",
       "      <td>0.953709</td>\n",
       "      <td>0.944490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.126700</td>\n",
       "      <td>0.339170</td>\n",
       "      <td>0.947587</td>\n",
       "      <td>0.951101</td>\n",
       "      <td>0.954141</td>\n",
       "      <td>0.948081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.102000</td>\n",
       "      <td>0.353412</td>\n",
       "      <td>0.944172</td>\n",
       "      <td>0.947603</td>\n",
       "      <td>0.956399</td>\n",
       "      <td>0.938967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.104600</td>\n",
       "      <td>0.352131</td>\n",
       "      <td>0.946993</td>\n",
       "      <td>0.950670</td>\n",
       "      <td>0.951327</td>\n",
       "      <td>0.950014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.068400</td>\n",
       "      <td>0.374303</td>\n",
       "      <td>0.943727</td>\n",
       "      <td>0.947295</td>\n",
       "      <td>0.954062</td>\n",
       "      <td>0.940624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Early Stopping] No improvement over best F1 for 3 consecutive epochs. Stopping training at epoch 23.\n",
      "[Early Stopping Info] Best F1: 0.9511 at epoch 20\n",
      "\n",
      "\n",
      "[GPU Info] Used 1 GPU(s), Current device: 0 (NVIDIA RTX 5000 Ada Generation)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='842' max='421' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [421/421 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Results for rank = 4:\n",
      "  Test Accuracy: 0.9474\n",
      "  Test F1: 0.9534\n",
      "  Validation Accuracy: 0.9476\n",
      "  Validation F1: 0.9511\n",
      "  Total Training Time: 3373.55s\n",
      "  Average Epoch Time: 140.98s\n",
      "  Best F1 Epoch: 20\n",
      "  Best Val F1: 0.9511\n",
      "  Early Stopped: True\n",
      "  Stop Info: epochs_without_improvement=3\n",
      "  Peak GPU Memory: 597.12 MB\n",
      "  LoRA Sparsity (<1e-3): 12.15%\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rank 4] Saved model to /hpc/group/xielab/hl385/LoRA/sparse_lora_rank4/final_model\n",
      "[Rank 4] Saved log history to log_history.json\n",
      "\n",
      "================================================================================\n",
      "Training Sparse LoRA DistilBERT with rank = 8, epochs = 30\n",
      "================================================================================\n",
      "[Rank 8] total params: 67,842,052\n",
      "[Rank 8] trainable params: 887,042\n",
      "[Rank 8] trainable params ratio (trainable / total): 1.3075%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33680' max='101040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 33680/101040 24:28 < 48:56, 22.94 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.311700</td>\n",
       "      <td>0.256966</td>\n",
       "      <td>0.923831</td>\n",
       "      <td>0.928322</td>\n",
       "      <td>0.939480</td>\n",
       "      <td>0.917426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.268800</td>\n",
       "      <td>0.255571</td>\n",
       "      <td>0.928285</td>\n",
       "      <td>0.932589</td>\n",
       "      <td>0.942720</td>\n",
       "      <td>0.922673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.274800</td>\n",
       "      <td>0.241609</td>\n",
       "      <td>0.936748</td>\n",
       "      <td>0.941014</td>\n",
       "      <td>0.943627</td>\n",
       "      <td>0.938415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.227300</td>\n",
       "      <td>0.261751</td>\n",
       "      <td>0.936600</td>\n",
       "      <td>0.941144</td>\n",
       "      <td>0.939461</td>\n",
       "      <td>0.942833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.227000</td>\n",
       "      <td>0.274559</td>\n",
       "      <td>0.940460</td>\n",
       "      <td>0.943955</td>\n",
       "      <td>0.955574</td>\n",
       "      <td>0.932615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.254500</td>\n",
       "      <td>0.254562</td>\n",
       "      <td>0.942390</td>\n",
       "      <td>0.946171</td>\n",
       "      <td>0.950655</td>\n",
       "      <td>0.941729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.214800</td>\n",
       "      <td>0.279685</td>\n",
       "      <td>0.942984</td>\n",
       "      <td>0.946888</td>\n",
       "      <td>0.948462</td>\n",
       "      <td>0.945319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.223400</td>\n",
       "      <td>0.296308</td>\n",
       "      <td>0.939124</td>\n",
       "      <td>0.944035</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.954985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.279003</td>\n",
       "      <td>0.941945</td>\n",
       "      <td>0.946269</td>\n",
       "      <td>0.941740</td>\n",
       "      <td>0.950842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.183400</td>\n",
       "      <td>0.289875</td>\n",
       "      <td>0.941351</td>\n",
       "      <td>0.945253</td>\n",
       "      <td>0.948804</td>\n",
       "      <td>0.941729</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Early Stopping] No improvement over best F1 for 3 consecutive epochs. Stopping training at epoch 10.\n",
      "[Early Stopping Info] Best F1: 0.9469 at epoch 7\n",
      "\n",
      "\n",
      "[GPU Info] Used 1 GPU(s), Current device: 0 (NVIDIA RTX 5000 Ada Generation)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='842' max='421' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [421/421 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Results for rank = 8:\n",
      "  Test Accuracy: 0.9433\n",
      "  Test F1: 0.9496\n",
      "  Validation Accuracy: 0.9430\n",
      "  Validation F1: 0.9469\n",
      "  Total Training Time: 1468.47s\n",
      "  Average Epoch Time: 140.97s\n",
      "  Best F1 Epoch: 7\n",
      "  Best Val F1: 0.9469\n",
      "  Early Stopped: True\n",
      "  Stop Info: epochs_without_improvement=3\n",
      "  Peak GPU Memory: 600.25 MB\n",
      "  LoRA Sparsity (<1e-3): 8.62%\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rank 8] Saved model to /hpc/group/xielab/hl385/LoRA/sparse_lora_rank8/final_model\n",
      "[Rank 8] Saved log history to log_history.json\n",
      "\n",
      "================================================================================\n",
      "Training Sparse LoRA DistilBERT with rank = 16, epochs = 30\n",
      "================================================================================\n",
      "[Rank 16] total params: 68,136,964\n",
      "[Rank 16] trainable params: 1,181,954\n",
      "[Rank 16] trainable params ratio (trainable / total): 1.7347%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='47152' max='101040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 47152/101040 33:45 < 38:34, 23.28 it/s, Epoch 14/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.354000</td>\n",
       "      <td>0.308009</td>\n",
       "      <td>0.921158</td>\n",
       "      <td>0.924989</td>\n",
       "      <td>0.946790</td>\n",
       "      <td>0.904170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.320100</td>\n",
       "      <td>0.302928</td>\n",
       "      <td>0.927097</td>\n",
       "      <td>0.932285</td>\n",
       "      <td>0.931129</td>\n",
       "      <td>0.933444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.319500</td>\n",
       "      <td>0.291822</td>\n",
       "      <td>0.934670</td>\n",
       "      <td>0.939676</td>\n",
       "      <td>0.933025</td>\n",
       "      <td>0.946424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.258800</td>\n",
       "      <td>0.311290</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.938518</td>\n",
       "      <td>0.930744</td>\n",
       "      <td>0.946424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.276500</td>\n",
       "      <td>0.317225</td>\n",
       "      <td>0.938085</td>\n",
       "      <td>0.941160</td>\n",
       "      <td>0.962204</td>\n",
       "      <td>0.921016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.277200</td>\n",
       "      <td>0.292271</td>\n",
       "      <td>0.941945</td>\n",
       "      <td>0.945536</td>\n",
       "      <td>0.953907</td>\n",
       "      <td>0.937310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.267400</td>\n",
       "      <td>0.294838</td>\n",
       "      <td>0.940757</td>\n",
       "      <td>0.944744</td>\n",
       "      <td>0.947500</td>\n",
       "      <td>0.942005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.242800</td>\n",
       "      <td>0.335551</td>\n",
       "      <td>0.933482</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.920711</td>\n",
       "      <td>0.958851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.242900</td>\n",
       "      <td>0.303215</td>\n",
       "      <td>0.944024</td>\n",
       "      <td>0.947486</td>\n",
       "      <td>0.955874</td>\n",
       "      <td>0.939243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.232000</td>\n",
       "      <td>0.307135</td>\n",
       "      <td>0.942390</td>\n",
       "      <td>0.946453</td>\n",
       "      <td>0.945931</td>\n",
       "      <td>0.946976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.233900</td>\n",
       "      <td>0.287152</td>\n",
       "      <td>0.945063</td>\n",
       "      <td>0.948568</td>\n",
       "      <td>0.954940</td>\n",
       "      <td>0.942281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.204300</td>\n",
       "      <td>0.304233</td>\n",
       "      <td>0.941945</td>\n",
       "      <td>0.946210</td>\n",
       "      <td>0.942708</td>\n",
       "      <td>0.949738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.169500</td>\n",
       "      <td>0.341317</td>\n",
       "      <td>0.942390</td>\n",
       "      <td>0.946379</td>\n",
       "      <td>0.947165</td>\n",
       "      <td>0.945595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.178100</td>\n",
       "      <td>0.323875</td>\n",
       "      <td>0.943875</td>\n",
       "      <td>0.947833</td>\n",
       "      <td>0.947310</td>\n",
       "      <td>0.948357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Early Stopping] No improvement over best F1 for 3 consecutive epochs. Stopping training at epoch 14.\n",
      "[Early Stopping Info] Best F1: 0.9486 at epoch 11\n",
      "\n",
      "\n",
      "[GPU Info] Used 1 GPU(s), Current device: 0 (NVIDIA RTX 5000 Ada Generation)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='842' max='421' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [421/421 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Results for rank = 16:\n",
      "  Test Accuracy: 0.9436\n",
      "  Test F1: 0.9495\n",
      "  Validation Accuracy: 0.9451\n",
      "  Validation F1: 0.9486\n",
      "  Total Training Time: 2025.79s\n",
      "  Average Epoch Time: 139.21s\n",
      "  Best F1 Epoch: 11\n",
      "  Best Val F1: 0.9486\n",
      "  Early Stopped: True\n",
      "  Stop Info: epochs_without_improvement=3\n",
      "  Peak GPU Memory: 604.68 MB\n",
      "  LoRA Sparsity (<1e-3): 22.17%\n",
      "================================================================================\n",
      "\n",
      "[Rank 16] Saved model to /hpc/group/xielab/hl385/LoRA/sparse_lora_rank16/final_model\n",
      "[Rank 16] Saved log history to log_history.json\n",
      "\n",
      "\n",
      "================================================================================\n",
      "GENERATING SUMMARY TABLE\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "BENCHMARK SUMMARY - Sparse LoRA\n",
      "================================================================================\n",
      " Rank Test Acc Test F1 Val Acc Val F1 Best Val F1  Best F1 Epoch Trainable Params Trainable % Total Time (s) Avg Epoch (s) Early Stopped Peak GPU (MB) Sparsity (<1e-3)\n",
      "    2   0.9424  0.9487  0.9422 0.9461      0.9461              8          665,858       0.98%        1627.23        142.58           Yes        590.62            2.86%\n",
      "    4   0.9474  0.9534  0.9476 0.9511      0.9511             20          739,586       1.09%        3373.55        140.98           Yes        597.12           12.15%\n",
      "    8   0.9433  0.9496  0.9430 0.9469      0.9469              7          887,042       1.31%        1468.47        140.97           Yes        600.25            8.62%\n",
      "   16   0.9436  0.9495  0.9451 0.9486      0.9486             11        1,181,954       1.73%        2025.79        139.21           Yes        604.68           22.17%\n",
      "\n",
      "✓ Summary saved to 'SparseLoRA_SST2_benchmark_summary.csv'\n",
      "✓ Complete results saved to 'sparse_lora_all_results.json'\n"
     ]
    }
   ],
   "source": [
    "# ================== SPARSE LoRA MODEL =================\n",
    "\n",
    "from typing import Dict, Any, List, Optional\n",
    "import math\n",
    "import json\n",
    "import pandas as pd\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# -------- Sparse LoRA config --------\n",
    "RANKS: List[int] = [2, 4, 8, 16]\n",
    "L1_LAMBDA = 1e-5   # sparsity strength for LoRA weights\n",
    "\n",
    "\n",
    "def count_trainable_params(model: torch.nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def count_total_params(model: torch.nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "\n",
    "def compute_lora_sparsity(model: torch.nn.Module, threshold: float = 1e-3) -> float:\n",
    "    \"\"\"\n",
    "    Approximate sparsity: fraction of LoRA parameters with |w| < threshold.\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    near_zero = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"lora_\" in name and param.requires_grad:\n",
    "            data = param.detach().abs()\n",
    "            total += data.numel()\n",
    "            near_zero += (data < threshold).sum().item()\n",
    "    return near_zero / total if total > 0 else math.nan\n",
    "\n",
    "\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "class MetricsCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    Callback to track metrics and implement early stopping.\n",
    "    \"\"\"\n",
    "    def __init__(self, early_stop_patience=3):\n",
    "        self.epoch_times = []\n",
    "        self.epoch_start_time = None\n",
    "        self.epoch_f1s = []\n",
    "        self.best_f1 = 0.0\n",
    "        self.best_f1_epoch = None\n",
    "        self.logged_epochs = set()\n",
    "        # Early stopping: stop when validation F1 fails to improve over best_f1 for `early_stop_patience` consecutive epochs\n",
    "        self.early_stop_patience = early_stop_patience\n",
    "        self.epochs_without_improvement = 0\n",
    "        self.early_stopped = False\n",
    "\n",
    "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
    "        self.epoch_start_time = time.time()\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        if self.epoch_start_time is not None:\n",
    "            epoch_time = time.time() - self.epoch_start_time\n",
    "            self.epoch_times.append(epoch_time)\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
    "        if 'eval_f1' in metrics and state.epoch > 0:\n",
    "            f1 = metrics['eval_f1']\n",
    "            current_epoch = int(state.epoch)\n",
    "            \n",
    "            if current_epoch not in self.logged_epochs:\n",
    "                self.logged_epochs.add(current_epoch)\n",
    "                self.epoch_f1s.append(f1)\n",
    "\n",
    "                # Update best F1 and reset counter if improved\n",
    "                if f1 > self.best_f1:\n",
    "                    self.best_f1 = f1\n",
    "                    self.best_f1_epoch = current_epoch\n",
    "                    self.epochs_without_improvement = 0\n",
    "                else:\n",
    "                    # F1 did not improve over best_f1\n",
    "                    self.epochs_without_improvement += 1\n",
    "\n",
    "                # Trigger early stopping if no improvement for patience epochs\n",
    "                if self.epochs_without_improvement >= self.early_stop_patience and not self.early_stopped:\n",
    "                    self.early_stopped = True\n",
    "                    # Request Trainer to stop training after this evaluation\n",
    "                    control.should_training_stop = True\n",
    "                    print(f\"\\n[Early Stopping] No improvement over best F1 for {self.epochs_without_improvement} consecutive epochs. Stopping training at epoch {current_epoch}.\")\n",
    "                    print(f\"[Early Stopping Info] Best F1: {self.best_f1:.4f} at epoch {self.best_f1_epoch}\\n\")\n",
    "\n",
    "\n",
    "class SparseLoraTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Trainer with L1 penalty only on LoRA parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self, l1_lambda: float = 0.0, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.l1_lambda = l1_lambda\n",
    "\n",
    "    def compute_loss(\n",
    "        self,\n",
    "        model,\n",
    "        inputs,\n",
    "        return_outputs: bool = False,\n",
    "        num_items_in_batch: Optional[int] = None,\n",
    "    ):\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        if self.l1_lambda > 0:\n",
    "            l1_reg = 0.0\n",
    "            for name, param in model.named_parameters():\n",
    "                if \"lora_\" in name and param.requires_grad:\n",
    "                    l1_reg = l1_reg + param.abs().sum()\n",
    "            loss = loss + self.l1_lambda * l1_reg\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "results_per_rank: List[Dict[str, Any]] = []\n",
    "\n",
    "for r in RANKS:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"Training Sparse LoRA DistilBERT with rank = {r}, epochs = {NUM_EPOCHS}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    set_all_seeds(SEED)\n",
    "\n",
    "    # Check for existing checkpoint\n",
    "    output_dir = f\"/hpc/group/xielab/hl385/LoRA/sparse_lora_rank{r}\"\n",
    "    checkpoint_dir = None\n",
    "    resume_from_checkpoint = False  # Set to True to resume from checkpoint\n",
    "    if resume_from_checkpoint and os.path.exists(output_dir):\n",
    "        checkpoints = [d for d in os.listdir(output_dir) if d.startswith(\"checkpoint-\")]\n",
    "        if checkpoints:\n",
    "            checkpoints.sort(key=lambda x: int(x.split(\"-\")[1]))\n",
    "            checkpoint_dir = os.path.join(output_dir, checkpoints[-1])\n",
    "            print(f\"Found existing checkpoint: {checkpoint_dir}\")\n",
    "            print(f\"Resuming training from this checkpoint...\\n\")\n",
    "\n",
    "    # Reset GPU memory stats\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    # Base DistilBERT for this rank\n",
    "    base_model = DistilBertForSequenceClassification.from_pretrained(\n",
    "        \"distilbert-base-uncased\",\n",
    "        num_labels=2,\n",
    "    )\n",
    "\n",
    "    # LoRA config: attention projections in DistilBERT\n",
    "    lora_config = LoraConfig(\n",
    "        r=r,\n",
    "        lora_alpha=2 * r,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=\"SEQ_CLS\",    # sequence classification\n",
    "        target_modules=[\"q_lin\", \"k_lin\", \"v_lin\", \"out_lin\"],\n",
    "    )\n",
    "\n",
    "    lora_model = get_peft_model(base_model, lora_config)\n",
    "    lora_model.to(DEVICE)\n",
    "\n",
    "    total_params = count_total_params(lora_model)\n",
    "    trainable_params = count_trainable_params(lora_model)\n",
    "    param_ratio = trainable_params / total_params\n",
    "\n",
    "    print(f\"[Rank {r}] total params: {total_params:,}\")\n",
    "    print(f\"[Rank {r}] trainable params: {trainable_params:,}\")\n",
    "    print(f\"[Rank {r}] trainable params ratio (trainable / total): {param_ratio:.4%}\")\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    training_args_lora = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        learning_rate=LR,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=100,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        greater_is_better=True,\n",
    "        save_total_limit=2,\n",
    "        seed=SEED,\n",
    "        report_to=\"none\",\n",
    "        disable_tqdm=False,\n",
    "    )\n",
    "\n",
    "    # Create metrics callback\n",
    "    metrics_callback = MetricsCallback(early_stop_patience=3)\n",
    "\n",
    "    trainer = SparseLoraTrainer(\n",
    "        l1_lambda=L1_LAMBDA,\n",
    "        model=lora_model,\n",
    "        args=training_args_lora,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[metrics_callback],\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "    trainer.train(resume_from_checkpoint=checkpoint_dir if checkpoint_dir else None)\n",
    "    end_time = time.time()\n",
    "    total_train_time = end_time - start_time\n",
    "    \n",
    "    # Calculate metrics\n",
    "    avg_epoch_time = sum(metrics_callback.epoch_times) / len(metrics_callback.epoch_times) if metrics_callback.epoch_times else 0.0\n",
    "    \n",
    "    # Get peak GPU memory\n",
    "    peak_gpu_memory_mb = 0.0\n",
    "    if torch.cuda.is_available():\n",
    "        peak_gpu_memory_mb = torch.cuda.max_memory_allocated() / (1024 ** 2)\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        current_device = torch.cuda.current_device()\n",
    "        device_name = torch.cuda.get_device_name(current_device)\n",
    "        print(f\"\\n[GPU Info] Used {num_gpus} GPU(s), Current device: {current_device} ({device_name})\")\n",
    "\n",
    "    # --- final evals (using best model loaded by load_best_model_at_end=True) ---\n",
    "    val_metrics = trainer.evaluate(eval_dataset=val_ds)\n",
    "    test_metrics = trainer.evaluate(eval_dataset=test_ds)\n",
    "    lora_sparsity = compute_lora_sparsity(lora_model, threshold=1e-3)\n",
    "    \n",
    "    # Print results in summary table format\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Results for rank = {r}:\")\n",
    "    print(f\"  Test Accuracy: {test_metrics.get('eval_accuracy', 0.0):.4f}\")\n",
    "    print(f\"  Test F1: {test_metrics.get('eval_f1', 0.0):.4f}\")\n",
    "    print(f\"  Validation Accuracy: {val_metrics.get('eval_accuracy', 0.0):.4f}\")\n",
    "    print(f\"  Validation F1: {val_metrics.get('eval_f1', 0.0):.4f}\")\n",
    "    print(f\"  Total Training Time: {total_train_time:.2f}s\")\n",
    "    print(f\"  Average Epoch Time: {avg_epoch_time:.2f}s\")\n",
    "    print(f\"  Best F1 Epoch: {metrics_callback.best_f1_epoch}\")\n",
    "    print(f\"  Best Val F1: {metrics_callback.best_f1:.4f}\")\n",
    "    print(f\"  Early Stopped: {metrics_callback.early_stopped}\")\n",
    "    if metrics_callback.early_stopped:\n",
    "        print(f\"  Stop Info: epochs_without_improvement={metrics_callback.epochs_without_improvement}\")\n",
    "    print(f\"  Peak GPU Memory: {peak_gpu_memory_mb:.2f} MB\")\n",
    "    print(f\"  LoRA Sparsity (<1e-3): {lora_sparsity:.2%}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "    # ==========================\n",
    "    # SAVE METRICS / MODEL / LOG\n",
    "    # ==========================\n",
    "    # 1) save metrics\n",
    "    metrics_payload = {\n",
    "        \"rank\": r,\n",
    "        \"total_train_time_sec\": total_train_time,\n",
    "        \"avg_epoch_time_sec\": avg_epoch_time,\n",
    "        \"best_f1_epoch\": metrics_callback.best_f1_epoch,\n",
    "        \"best_val_f1\": float(metrics_callback.best_f1),\n",
    "        \"early_stopped\": metrics_callback.early_stopped,\n",
    "        \"peak_gpu_memory_mb\": peak_gpu_memory_mb,\n",
    "        \"total_params\": int(total_params),\n",
    "        \"trainable_params\": int(trainable_params),\n",
    "        \"param_ratio\": float(param_ratio),\n",
    "        \"lora_sparsity_<1e-3\": float(lora_sparsity),\n",
    "        \"val_metrics\": val_metrics,\n",
    "        \"test_metrics\": test_metrics,\n",
    "    }\n",
    "    with open(os.path.join(output_dir, \"final_metrics.json\"), \"w\") as f:\n",
    "        json.dump(metrics_payload, f, indent=4)\n",
    "\n",
    "    # 2) save final model (best checkpoint)\n",
    "    final_model_dir = os.path.join(output_dir, \"final_model\")\n",
    "    trainer.save_model(final_model_dir)  # saves model + config\n",
    "    tokenizer.save_pretrained(final_model_dir)  # save tokenizer too\n",
    "    print(f\"[Rank {r}] Saved model to {final_model_dir}\")\n",
    "\n",
    "    # 3) save training history\n",
    "    log_history = trainer.state.log_history\n",
    "    with open(os.path.join(output_dir, \"log_history.json\"), \"w\") as f:\n",
    "        json.dump(log_history, f, indent=4)\n",
    "    print(f\"[Rank {r}] Saved log history to log_history.json\")\n",
    "\n",
    "    # --- store in-memory summary for printing ---\n",
    "    results_per_rank.append(\n",
    "        {\n",
    "            \"rank\": r,\n",
    "            \"final_test_accuracy\": test_metrics.get('eval_accuracy', 0.0),\n",
    "            \"final_test_f1\": test_metrics.get('eval_f1', 0.0),\n",
    "            \"final_val_accuracy\": val_metrics.get('eval_accuracy', 0.0),\n",
    "            \"final_val_f1\": val_metrics.get('eval_f1', 0.0),\n",
    "            \"total_parameters\": total_params,\n",
    "            \"trainable_parameters\": trainable_params,\n",
    "            \"trainable_percentage\": param_ratio * 100,\n",
    "            \"total_training_time\": total_train_time,\n",
    "            \"average_epoch_time\": avg_epoch_time,\n",
    "            \"peak_gpu_memory_mb\": peak_gpu_memory_mb,\n",
    "            \"sparsity\": lora_sparsity,\n",
    "            \"best_f1_epoch\": metrics_callback.best_f1_epoch,\n",
    "            \"best_val_f1\": metrics_callback.best_f1,\n",
    "            \"early_stopped\": metrics_callback.early_stopped,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Generate and save summary table\n",
    "print(\"\\n\\n\" + \"=\" * 80)\n",
    "print(\"GENERATING SUMMARY TABLE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "summary_data = []\n",
    "for result in results_per_rank:\n",
    "    summary_data.append({\n",
    "        \"Rank\": result['rank'],\n",
    "        \"Test Acc\": f\"{result['final_test_accuracy']:.4f}\",\n",
    "        \"Test F1\": f\"{result['final_test_f1']:.4f}\",\n",
    "        \"Val Acc\": f\"{result['final_val_accuracy']:.4f}\",\n",
    "        \"Val F1\": f\"{result['final_val_f1']:.4f}\",\n",
    "        \"Best Val F1\": f\"{result['best_val_f1']:.4f}\",\n",
    "        \"Best F1 Epoch\": result['best_f1_epoch'],\n",
    "        \"Trainable Params\": f\"{result['trainable_parameters']:,}\",\n",
    "        \"Trainable %\": f\"{result['trainable_percentage']:.2f}%\",\n",
    "        \"Total Time (s)\": f\"{result['total_training_time']:.2f}\",\n",
    "        \"Avg Epoch (s)\": f\"{result['average_epoch_time']:.2f}\",\n",
    "        \"Early Stopped\": \"Yes\" if result['early_stopped'] else \"No\",\n",
    "        \"Peak GPU (MB)\": f\"{result['peak_gpu_memory_mb']:.2f}\",\n",
    "        \"Sparsity (<1e-3)\": f\"{result['sparsity']*100:.2f}%\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BENCHMARK SUMMARY - Sparse LoRA\")\n",
    "print(\"=\"*80)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Save summary to CSV\n",
    "csv_filename = \"SparseLoRA_SST2_benchmark_summary.csv\"\n",
    "summary_df.to_csv(csv_filename, index=False)\n",
    "print(f\"\\n✓ Summary saved to '{csv_filename}'\")\n",
    "\n",
    "# Save complete results\n",
    "with open(\"sparse_lora_all_results.json\", \"w\") as f:\n",
    "    json.dump(results_per_rank, f, indent=2)\n",
    "print(\"✓ Complete results saved to 'sparse_lora_all_results.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1d6b99-1ac4-43c4-9fad-55a184209bd8",
   "metadata": {},
   "source": [
    "### IMDB Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8b7334-2fdd-4959-ba29-f97654cf3c01",
   "metadata": {},
   "source": [
    "Have commented out in the corresponding section \\\n",
    "Just to replace the loaded dataset accordingly:\n",
    "- DATASET = \"imdb\"\n",
    "- TEXT_COL = \"text\"\n",
    "- LABEL_COL = \"label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317de448-2bfd-4546-ad32-565dd04594f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece685",
   "language": "python",
   "name": "ece685"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
