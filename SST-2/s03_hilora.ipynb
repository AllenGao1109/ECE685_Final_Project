{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19244fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional\n",
    "import os\n",
    "\n",
    "# Force to use only GPU 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "class HiRALayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        r: int = 32,\n",
    "        lora_alpha: int = 32,\n",
    "        lora_dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.r = r\n",
    "        self.lora_alpha = lora_alpha\n",
    "        self.scaling = lora_alpha / r\n",
    "        \n",
    "        self.lora_A = nn.Parameter(torch.zeros(in_features, r))\n",
    "        self.lora_B = nn.Parameter(torch.randn(r, out_features))\n",
    "        \n",
    "        nn.init.zeros_(self.lora_A)\n",
    "        nn.init.kaiming_uniform_(self.lora_B, a=0)\n",
    "        \n",
    "        self.lora_dropout = nn.Dropout(p=lora_dropout) if lora_dropout > 0 else nn.Identity()\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, W0: torch.Tensor) -> torch.Tensor:\n",
    "        result = F.linear(x, W0)\n",
    "        \n",
    "        lora_update = self.lora_A @ self.lora_B  # [in_features, out_features]\n",
    "        \n",
    "        hadamard_update = W0.T * lora_update  # [in_features, out_features]\n",
    "        \n",
    "        result += self.lora_dropout(x @ hadamard_update) * self.scaling\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "class HiRALinear(nn.Module):\n",
    "    def __init__(self, linear_layer: nn.Linear, r: int = 32, lora_alpha: int = 32):\n",
    "        super().__init__()\n",
    "        self.linear = linear_layer\n",
    "        self.hira = HiRALayer(\n",
    "            in_features=linear_layer.in_features,\n",
    "            out_features=linear_layer.out_features,\n",
    "            r=r,\n",
    "            lora_alpha=lora_alpha\n",
    "        )\n",
    "        self.linear.weight.requires_grad = False\n",
    "        if self.linear.bias is not None:\n",
    "            self.linear.bias.requires_grad = False\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.hira(x, self.linear.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec2120ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dc10860",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/group/xielab/hl385/miniconda3/envs/ece685/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "import torch.nn as nn\n",
    "\n",
    "def apply_hira_to_model(model, r=32, lora_alpha=32, target_modules=['q_lin', 'k_lin', 'v_lin', 'out_lin', 'ffn.lin1', 'ffn.lin2']):\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if any(target in name for target in target_modules):\n",
    "                parent_name = '.'.join(name.split('.')[:-1])\n",
    "                attr_name = name.split('.')[-1]\n",
    "                \n",
    "                if parent_name:\n",
    "                    parent_module = model.get_submodule(parent_name)\n",
    "                else:\n",
    "                    parent_module = model\n",
    "                \n",
    "                hira_layer = HiRALinear(module, r=r, lora_alpha=lora_alpha)\n",
    "                setattr(parent_module, attr_name, hira_layer)\n",
    "                \n",
    "                print(f\"Applied HiRA to: {name}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    return trainable_params, all_params, 100 * trainable_params / all_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c764b4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "def load_sst2_data(tokenizer, max_length=128):\n",
    "    \"\"\"8:1:1 \"\"\"\n",
    "    raw = load_dataset(\"glue\", \"sst2\")  # 有 train / validation / test[web:110]\n",
    "\n",
    "    train_valid = raw[\"train\"]\n",
    "    train_valid = train_valid.shuffle(seed=42)\n",
    "    n = len(train_valid)\n",
    "    n_train = int(0.8 * n)\n",
    "    n_val = int(0.1 * n)\n",
    "    n_test = n - n_train - n_val\n",
    "\n",
    "    train_dataset = train_valid.select(range(n_train))\n",
    "    val_dataset   = train_valid.select(range(n_train, n_train + n_val))\n",
    "    test_dataset  = train_valid.select(range(n_train + n_val, n))\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        enc = tokenizer(\n",
    "            examples[\"sentence\"],\n",
    "            truncation=True,\n",
    "            max_length=max_length\n",
    "        )\n",
    "        enc[\"labels\"] = examples[\"label\"]\n",
    "        return enc\n",
    "\n",
    "    train_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=train_dataset.column_names)\n",
    "    val_dataset   = val_dataset.map(preprocess_function,   batched=True, remove_columns=val_dataset.column_names)\n",
    "    test_dataset  = test_dataset.map(preprocess_function,  batched=True, remove_columns=test_dataset.column_names)\n",
    "\n",
    "    return {\n",
    "        \"train\": train_dataset,\n",
    "        \"validation\": val_dataset,\n",
    "        \"test\": test_dataset,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def load_imdb_data(tokenizer, max_length=256):\n",
    "\n",
    "    raw = load_dataset(\"imdb\")  \n",
    "\n",
    "    train_full = raw[\"train\"]               \n",
    "    train_full = train_full.shuffle(seed=42)\n",
    "    n = len(train_full)\n",
    "    n_train = int(0.8 * n)\n",
    "    n_val = int(0.1 * n)\n",
    "    n_test = n - n_train - n_val\n",
    "\n",
    "    train_dataset = train_full.select(range(n_train))\n",
    "    val_dataset   = train_full.select(range(n_train, n_train + n_val))\n",
    "    test_dataset  = train_full.select(range(n_train + n_val, n))\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        enc = tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "        )\n",
    "        enc[\"labels\"] = examples[\"label\"]\n",
    "        return enc\n",
    "\n",
    "    train_dataset = train_dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        remove_columns=train_dataset.column_names,\n",
    "    )\n",
    "    val_dataset = val_dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        remove_columns=val_dataset.column_names,\n",
    "    )\n",
    "    test_dataset = test_dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        remove_columns=test_dataset.column_names,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"train\": train_dataset,\n",
    "        \"validation\": val_dataset,\n",
    "        \"test\": test_dataset,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bf0561",
   "metadata": {},
   "source": [
    "# After changing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08d17063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    DistilBertForSequenceClassification,\n",
    "    DistilBertTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_params = 0\n",
    "    for p in model.parameters():\n",
    "        num = p.numel()\n",
    "        all_params += num\n",
    "        if p.requires_grad:\n",
    "            trainable_params += num\n",
    "    percentage = 100.0 * trainable_params / all_params if all_params > 0 else 0.0\n",
    "    return trainable_params, all_params, percentage\n",
    "\n",
    "\n",
    "def get_model_sparsity(model, threshold: float = 1e-3) -> float:\n",
    "    total_elems = 0\n",
    "    small_elems = 0\n",
    "    for p in model.parameters():\n",
    "        if p is None:\n",
    "            continue\n",
    "        data = p.detach()\n",
    "        total_elems += data.numel()\n",
    "        small_elems += (data.abs() < threshold).sum().item()\n",
    "    if total_elems == 0:\n",
    "        return 0.0\n",
    "    return small_elems / total_elems\n",
    "\n",
    "def train_hira_model(\n",
    "    dataset_name: str = \"sst2\",\n",
    "    model_name: str = \"distilbert-base-uncased\",\n",
    "    r: int = 32,\n",
    "    lora_alpha: int = 32,\n",
    "    num_epochs: int = 30,\n",
    "    batch_size: int = 16,\n",
    "    learning_rate: float = 5e-4,\n",
    "    weight_decay: float = 0.01,\n",
    "    warmup_steps: int = 100,\n",
    "    logging_steps: int = 100,\n",
    "    max_length: int = 128,\n",
    "    early_stop_patience: int = 3,\n",
    "    output_dir: str = \"./results_hira\",\n",
    "    resume_from_checkpoint: bool = False,\n",
    "):\n",
    "\n",
    "    device = torch.device(\n",
    "        \"cuda\"\n",
    "        if torch.cuda.is_available()\n",
    "        else \"mps\"\n",
    "        if torch.backends.mps.is_available()\n",
    "        else \"cpu\"\n",
    "    )\n",
    "    print(f\"[{dataset_name}][r={r}] Using device: {device}\")\n",
    "\n",
    "    # Check for existing checkpoint in /hpc/group/xielab/hl385/LoRA\n",
    "    checkpoint_path = os.path.join(\"/hpc/group/xielab/hl385/LoRA\", f\"checkpoint_{dataset_name}_r{r}.pt\")\n",
    "    start_epoch = 0\n",
    "    if resume_from_checkpoint and os.path.exists(checkpoint_path):\n",
    "        print(f\"[{dataset_name}][r={r}] Found checkpoint: {checkpoint_path}\")\n",
    "        print(f\"[{dataset_name}][r={r}] Resuming training from checkpoint...\\n\")\n",
    "    else:\n",
    "        checkpoint_path = None\n",
    "\n",
    "    # Reset peak GPU memory stats\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "    data_collator = DataCollatorWithPadding(\n",
    "        tokenizer=tokenizer,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    num_labels = 2\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=num_labels,\n",
    "    )\n",
    "    model = apply_hira_to_model(model, r=r, lora_alpha=lora_alpha)\n",
    "    model = model.to(device)\n",
    "\n",
    "    trainable_params, all_params, percentage = get_trainable_parameters(model)\n",
    "    print(\n",
    "        f\"[{dataset_name}][r={r}] Trainable params: {trainable_params:,} || \"\n",
    "        f\"All params: {all_params:,} || Trainable%: {percentage:.4f}%\"\n",
    "    )\n",
    "\n",
    "    if dataset_name == \"sst2\":\n",
    "        dataset = load_sst2_data(tokenizer, max_length)\n",
    "    elif dataset_name == \"imdb\":\n",
    "        dataset = load_imdb_data(tokenizer, max_length)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset_name: {dataset_name}\")\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        dataset[\"train\"],\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=data_collator,\n",
    "    )\n",
    "    val_split_name = \"validation\" if \"validation\" in dataset else \"test\"\n",
    "    val_dataloader = DataLoader(\n",
    "        dataset[val_split_name],\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=data_collator,\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        dataset[\"test\"],\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=data_collator,\n",
    "    )\n",
    "\n",
    "    optimizer = AdamW(\n",
    "        [p for p in model.parameters() if p.requires_grad],\n",
    "        lr=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "    )\n",
    "    total_steps = len(train_dataloader) * num_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_steps,\n",
    "    )\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Load checkpoint if resuming\n",
    "    if checkpoint_path is not None:\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "        start_epoch = checkpoint[\"epoch\"]\n",
    "        best_val_f1 = checkpoint[\"best_val_f1\"]\n",
    "        best_val_accuracy = checkpoint[\"best_val_accuracy\"]\n",
    "        best_epoch = checkpoint[\"best_epoch\"]\n",
    "        print(f\"[{dataset_name}][r={r}] Resumed from epoch {start_epoch}\")\n",
    "        print(f\"[{dataset_name}][r={r}] Best F1 so far: {best_val_f1:.4f} at epoch {best_epoch}\\n\")\n",
    "    else:\n",
    "        best_val_accuracy = 0.0\n",
    "        best_val_f1 = 0.0\n",
    "        best_epoch = 0\n",
    "\n",
    "    epoch_times = []\n",
    "    total_train_time = 0.0\n",
    "    epochs_without_improvement = 0\n",
    "    early_stopped = False\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        start_t = time.perf_counter()\n",
    "\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        progress_bar = tqdm(\n",
    "            train_dataloader,\n",
    "            desc=f\"[{dataset_name}][r={r}] Epoch {epoch + 1}/{num_epochs}\",\n",
    "            ncols=100,\n",
    "            leave=False,\n",
    "        )\n",
    "        for step, batch in enumerate(progress_bar):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            avg_loss = total_loss / (step + 1)\n",
    "            progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\", \"avg_loss\": f\"{avg_loss:.4f}\"})\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        # Validation\n",
    "        acc_metric = evaluate.load(\"accuracy\")\n",
    "        f1_metric = evaluate.load(\"f1\")\n",
    "        model.eval()\n",
    "        for batch in val_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "            acc_metric.add_batch(predictions=preds, references=batch[\"labels\"])\n",
    "            f1_metric.add_batch(predictions=preds, references=batch[\"labels\"])\n",
    "        val_acc = acc_metric.compute()[\"accuracy\"]\n",
    "        val_f1 = f1_metric.compute()[\"f1\"]\n",
    "\n",
    "        end_t = time.perf_counter()\n",
    "        epoch_time = end_t - start_t\n",
    "        epoch_times.append(epoch_time)\n",
    "        total_train_time += epoch_time\n",
    "\n",
    "        print(\n",
    "            f\"[{dataset_name}][r={r}] Epoch {epoch + 1} | \"\n",
    "            f\"train_loss={avg_train_loss:.4f} | \"\n",
    "            f\"val_acc={val_acc:.4f} | \"\n",
    "            f\"val_f1={val_f1:.4f} | \"\n",
    "            f\"time={epoch_time:.2f}s\"\n",
    "        )\n",
    "\n",
    "        # Update best model and reset counter if improved\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            best_val_accuracy = val_acc\n",
    "            best_epoch = epoch + 1\n",
    "            epochs_without_improvement = 0\n",
    "\n",
    "            save_path = os.path.join(\"/hpc/group/xielab/hl385/LoRA\", f\"best_model_hira_{dataset_name}_r{r}.pt\")\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"val_f1\": best_val_f1,\n",
    "                    \"val_accuracy\": best_val_accuracy,\n",
    "                },\n",
    "                save_path,\n",
    "            )\n",
    "        else:\n",
    "            # F1 did not exceed best_val_f1\n",
    "            epochs_without_improvement += 1\n",
    "        \n",
    "        # Early stopping: stop when no improvement for patience epochs\n",
    "        if epochs_without_improvement >= early_stop_patience:\n",
    "            early_stopped = True\n",
    "            print(\n",
    "                f\"\\n[Early Stopping] No improvement over best F1 for {early_stop_patience} consecutive epochs. \"\n",
    "                f\"Stopped at epoch {epoch + 1}. Best F1: {best_val_f1:.4f} at epoch {best_epoch}\\n\"\n",
    "            )\n",
    "            break\n",
    "        \n",
    "        # Save checkpoint to /hpc/group/xielab/hl385/LoRA\n",
    "        checkpoint_save_path = os.path.join(\"/hpc/group/xielab/hl385/LoRA\", f\"checkpoint_hira_{dataset_name}_r{r}.pt\")\n",
    "        torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "                \"best_val_f1\": best_val_f1,\n",
    "                \"best_val_accuracy\": best_val_accuracy,\n",
    "                \"best_epoch\": best_epoch,\n",
    "            },\n",
    "            checkpoint_save_path,\n",
    "        )\n",
    "    \n",
    "    avg_time_per_epoch = sum(epoch_times) / len(epoch_times) if epoch_times else 0.0\n",
    "    \n",
    "    # Get peak GPU memory\n",
    "    peak_gpu_memory_mb = 0.0\n",
    "    if torch.cuda.is_available():\n",
    "        peak_gpu_memory_mb = torch.cuda.max_memory_allocated() / (1024 ** 2)\n",
    "    \n",
    "    print(\n",
    "        f\"[{dataset_name}][r={r}] Training completed! \"\n",
    "        f\"best_val_acc={best_val_accuracy:.4f}, \"\n",
    "        f\"best_val_f1={best_val_f1:.4f}, \"\n",
    "        f\"avg_time/epoch={avg_time_per_epoch:.2f}s, \"\n",
    "        f\"best_epoch={best_epoch}, \"\n",
    "        f\"early_stopped={early_stopped}, \"\n",
    "        f\"peak_gpu_memory={peak_gpu_memory_mb:.2f}MB, \"\n",
    "        f\"trainable_params={trainable_params}, \"\n",
    "        f\"trainable_ratio={percentage:.4f}%\"\n",
    "    )\n",
    "\n",
    "    # Load best model for testing\n",
    "    best_model_path = os.path.join(output_dir, f\"best_model_r{r}.pt\")\n",
    "    if os.path.exists(best_model_path):\n",
    "        checkpoint = torch.load(best_model_path)\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        print(f\"[{dataset_name}][r={r}] Loaded best model from {best_model_path}\")\n",
    "    else:\n",
    "        print(f\"[{dataset_name}][r={r}] Warning: Best model not found, using current model\")\n",
    "\n",
    "    # Test evaluation\n",
    "    acc_metric = evaluate.load(\"accuracy\")\n",
    "    f1_metric = evaluate.load(\"f1\")\n",
    "    model.eval()\n",
    "    for batch in test_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        preds = torch.argmax(outputs.logits, dim=-1)\n",
    "        acc_metric.add_batch(predictions=preds, references=batch[\"labels\"])\n",
    "        f1_metric.add_batch(predictions=preds, references=batch[\"labels\"])\n",
    "\n",
    "    test_acc = acc_metric.compute()[\"accuracy\"]\n",
    "    test_f1 = f1_metric.compute()[\"f1\"]\n",
    "    print(f\"[{dataset_name}][r={r}] Final test accuracy: {test_acc:.4f}, test_f1={test_f1:.4f}\")\n",
    "\n",
    "    sparsity = get_model_sparsity(model, threshold=1e-3)\n",
    "\n",
    "    return {\n",
    "        \"dataset\": dataset_name,\n",
    "        \"rank\": r,\n",
    "        \"final_test_accuracy\": test_acc,\n",
    "        \"final_test_f1\": test_f1,\n",
    "        \"final_val_accuracy\": best_val_accuracy,\n",
    "        \"final_val_f1\": best_val_f1,\n",
    "        \"total_parameters\": all_params,\n",
    "        \"trainable_parameters\": trainable_params,\n",
    "        \"trainable_percentage\": percentage,\n",
    "        \"total_training_time\": total_train_time,\n",
    "        \"average_epoch_time\": avg_time_per_epoch,\n",
    "        \"best_f1_epoch\": best_epoch,\n",
    "        \"best_val_f1\": best_val_f1,\n",
    "        \"peak_gpu_memory_mb\": peak_gpu_memory_mb,\n",
    "        \"sparsity\": sparsity,\n",
    "        \"early_stopped\": early_stopped,\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_summary_table(results, save_prefix=\"HiRA\"):\n",
    "    \"\"\"Generate and save summary table with all metrics\"\"\"\n",
    "    \n",
    "    summary_data = []\n",
    "    for result in results:\n",
    "        summary_data.append({\n",
    "            \"Rank\": result[\"rank\"],\n",
    "            \"Test Acc\": f\"{result['final_test_accuracy']:.4f}\",\n",
    "            \"Test F1\": f\"{result['final_test_f1']:.4f}\",\n",
    "            \"Val Acc\": f\"{result['final_val_accuracy']:.4f}\",\n",
    "            \"Val F1\": f\"{result['final_val_f1']:.4f}\",\n",
    "            \"Best Val F1\": f\"{result['best_val_f1']:.4f}\",\n",
    "            \"Best F1 Epoch\": result['best_f1_epoch'],\n",
    "            \"Trainable Params\": f\"{result['trainable_parameters']:,}\",\n",
    "            \"Trainable %\": f\"{result['trainable_percentage']:.2f}%\",\n",
    "            \"Total Time (s)\": f\"{result['total_training_time']:.2f}\",\n",
    "            \"Avg Epoch (s)\": f\"{result['average_epoch_time']:.2f}\",\n",
    "            \"Early Stopped\": \"Yes\" if result['early_stopped'] else \"No\",\n",
    "            \"Peak GPU (MB)\": f\"{result['peak_gpu_memory_mb']:.2f}\",\n",
    "            \"Sparsity (<1e-3)\": f\"{result['sparsity']*100:.2f}%\",\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    # Print summary table\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"BENCHMARK SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(summary_df.to_string(index=False))\n",
    "    \n",
    "    # Save summary to CSV\n",
    "    csv_filename = f\"{save_prefix}_benchmark_summary.csv\"\n",
    "    summary_df.to_csv(csv_filename, index=False)\n",
    "    print(f\"\\n✓ Summary saved to '{csv_filename}'\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6d20232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Training HiRA on SST-2 with different ranks\n",
      "==================================================\n",
      "\n",
      "------------------------------\n",
      "HiRA with rank r=2\n",
      "------------------------------\n",
      "[sst2][r=2] Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied HiRA to: distilbert.transformer.layer.0.attention.q_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.0.attention.k_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.0.attention.v_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.0.attention.out_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.0.ffn.lin1\n",
      "Applied HiRA to: distilbert.transformer.layer.0.ffn.lin2\n",
      "Applied HiRA to: distilbert.transformer.layer.1.attention.q_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.1.attention.k_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.1.attention.v_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.1.attention.out_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.1.ffn.lin1\n",
      "Applied HiRA to: distilbert.transformer.layer.1.ffn.lin2\n",
      "Applied HiRA to: distilbert.transformer.layer.2.attention.q_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.2.attention.k_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.2.attention.v_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.2.attention.out_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.2.ffn.lin1\n",
      "Applied HiRA to: distilbert.transformer.layer.2.ffn.lin2\n",
      "Applied HiRA to: distilbert.transformer.layer.3.attention.q_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.3.attention.k_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.3.attention.v_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.3.attention.out_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.3.ffn.lin1\n",
      "Applied HiRA to: distilbert.transformer.layer.3.ffn.lin2\n",
      "Applied HiRA to: distilbert.transformer.layer.4.attention.q_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.4.attention.k_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.4.attention.v_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.4.attention.out_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.4.ffn.lin1\n",
      "Applied HiRA to: distilbert.transformer.layer.4.ffn.lin2\n",
      "Applied HiRA to: distilbert.transformer.layer.5.attention.q_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.5.attention.k_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.5.attention.v_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.5.attention.out_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.5.ffn.lin1\n",
      "Applied HiRA to: distilbert.transformer.layer.5.ffn.lin2\n",
      "[sst2][r=2] Trainable params: 24,612,098 || All params: 67,120,898 || Trainable%: 36.6683%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sst2][r=2] Epoch 1 | train_loss=0.2431 | val_acc=0.9307 | val_f1=0.9369 | time=73.12s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sst2][r=2] Epoch 2 | train_loss=0.0996 | val_acc=0.9361 | val_f1=0.9430 | time=70.74s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sst2][r=2] Epoch 3 | train_loss=0.0574 | val_acc=0.9379 | val_f1=0.9444 | time=74.40s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sst2][r=2] Epoch 4 | train_loss=0.0367 | val_acc=0.9373 | val_f1=0.9442 | time=73.94s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sst2][r=2] Epoch 5 | train_loss=0.0285 | val_acc=0.9329 | val_f1=0.9398 | time=73.87s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sst2][r=2] Epoch 6 | train_loss=0.0205 | val_acc=0.9330 | val_f1=0.9401 | time=71.63s\n",
      "\n",
      "[Early Stopping] No improvement over best F1 for 3 consecutive epochs. Stopped at epoch 6. Best F1: 0.9444 at epoch 3\n",
      "\n",
      "[sst2][r=2] Training completed! best_val_acc=0.9379, best_val_f1=0.9444, avg_time/epoch=72.95s, best_epoch=3, early_stopped=True, peak_gpu_memory=1384.98MB, trainable_params=24612098, trainable_ratio=36.6683%\n",
      "[sst2][r=2] Warning: Best model not found, using current model\n",
      "[sst2][r=2] Final test accuracy: 0.9363, test_f1=0.9439\n",
      "\n",
      "------------------------------\n",
      "HiRA with rank r=4\n",
      "------------------------------\n",
      "[sst2][r=4] Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied HiRA to: distilbert.transformer.layer.0.attention.q_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.0.attention.k_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.0.attention.v_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.0.attention.out_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.0.ffn.lin1\n",
      "Applied HiRA to: distilbert.transformer.layer.0.ffn.lin2\n",
      "Applied HiRA to: distilbert.transformer.layer.1.attention.q_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.1.attention.k_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.1.attention.v_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.1.attention.out_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.1.ffn.lin1\n",
      "Applied HiRA to: distilbert.transformer.layer.1.ffn.lin2\n",
      "Applied HiRA to: distilbert.transformer.layer.2.attention.q_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.2.attention.k_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.2.attention.v_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.2.attention.out_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.2.ffn.lin1\n",
      "Applied HiRA to: distilbert.transformer.layer.2.ffn.lin2\n",
      "Applied HiRA to: distilbert.transformer.layer.3.attention.q_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.3.attention.k_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.3.attention.v_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.3.attention.out_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.3.ffn.lin1\n",
      "Applied HiRA to: distilbert.transformer.layer.3.ffn.lin2\n",
      "Applied HiRA to: distilbert.transformer.layer.4.attention.q_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.4.attention.k_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.4.attention.v_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.4.attention.out_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.4.ffn.lin1\n",
      "Applied HiRA to: distilbert.transformer.layer.4.ffn.lin2\n",
      "Applied HiRA to: distilbert.transformer.layer.5.attention.q_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.5.attention.k_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.5.attention.v_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.5.attention.out_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.5.ffn.lin1\n",
      "Applied HiRA to: distilbert.transformer.layer.5.ffn.lin2\n",
      "[sst2][r=4] Trainable params: 24,777,986 || All params: 67,286,786 || Trainable%: 36.8244%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sst2][r=4] Epoch 1 | train_loss=0.2436 | val_acc=0.9369 | val_f1=0.9443 | time=73.82s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sst2][r=4] Epoch 2 | train_loss=0.1023 | val_acc=0.9376 | val_f1=0.9445 | time=72.03s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sst2][r=4] Epoch 3 | train_loss=0.0583 | val_acc=0.9373 | val_f1=0.9442 | time=73.71s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sst2][r=4] Epoch 4 | train_loss=0.0390 | val_acc=0.9372 | val_f1=0.9439 | time=73.12s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sst2][r=4] Epoch 5 | train_loss=0.0274 | val_acc=0.9333 | val_f1=0.9409 | time=73.60s\n",
      "\n",
      "[Early Stopping] No improvement over best F1 for 3 consecutive epochs. Stopped at epoch 5. Best F1: 0.9445 at epoch 2\n",
      "\n",
      "[sst2][r=4] Training completed! best_val_acc=0.9376, best_val_f1=0.9445, avg_time/epoch=73.25s, best_epoch=2, early_stopped=True, peak_gpu_memory=1835.24MB, trainable_params=24777986, trainable_ratio=36.8244%\n",
      "[sst2][r=4] Warning: Best model not found, using current model\n",
      "[sst2][r=4] Final test accuracy: 0.9350, test_f1=0.9430\n",
      "\n",
      "------------------------------\n",
      "HiRA with rank r=8\n",
      "------------------------------\n",
      "[sst2][r=8] Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied HiRA to: distilbert.transformer.layer.0.attention.q_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.0.attention.k_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.0.attention.v_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.0.attention.out_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.0.ffn.lin1\n",
      "Applied HiRA to: distilbert.transformer.layer.0.ffn.lin2\n",
      "Applied HiRA to: distilbert.transformer.layer.1.attention.q_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.1.attention.k_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.1.attention.v_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.1.attention.out_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.1.ffn.lin1\n",
      "Applied HiRA to: distilbert.transformer.layer.1.ffn.lin2\n",
      "Applied HiRA to: distilbert.transformer.layer.2.attention.q_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.2.attention.k_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.2.attention.v_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.2.attention.out_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.2.ffn.lin1\n",
      "Applied HiRA to: distilbert.transformer.layer.2.ffn.lin2\n",
      "Applied HiRA to: distilbert.transformer.layer.3.attention.q_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.3.attention.k_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.3.attention.v_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.3.attention.out_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.3.ffn.lin1\n",
      "Applied HiRA to: distilbert.transformer.layer.3.ffn.lin2\n",
      "Applied HiRA to: distilbert.transformer.layer.4.attention.q_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.4.attention.k_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.4.attention.v_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.4.attention.out_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.4.ffn.lin1\n",
      "Applied HiRA to: distilbert.transformer.layer.4.ffn.lin2\n",
      "Applied HiRA to: distilbert.transformer.layer.5.attention.q_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.5.attention.k_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.5.attention.v_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.5.attention.out_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.5.ffn.lin1\n",
      "Applied HiRA to: distilbert.transformer.layer.5.ffn.lin2\n",
      "[sst2][r=8] Trainable params: 25,109,762 || All params: 67,618,562 || Trainable%: 37.1344%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sst2][r=8] Epoch 1 | train_loss=0.2416 | val_acc=0.9342 | val_f1=0.9424 | time=74.42s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sst2][r=8] Epoch 2 | train_loss=0.1017 | val_acc=0.9378 | val_f1=0.9444 | time=73.71s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sst2][r=8] Epoch 3 | train_loss=0.0560 | val_acc=0.9402 | val_f1=0.9470 | time=72.26s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sst2][r=8] Epoch 4 | train_loss=0.0383 | val_acc=0.9378 | val_f1=0.9445 | time=74.21s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sst2][r=8] Epoch 5 | train_loss=0.0265 | val_acc=0.9339 | val_f1=0.9418 | time=73.86s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sst2][r=8] Epoch 6 | train_loss=0.0207 | val_acc=0.9347 | val_f1=0.9414 | time=73.27s\n",
      "\n",
      "[Early Stopping] No improvement over best F1 for 3 consecutive epochs. Stopped at epoch 6. Best F1: 0.9470 at epoch 3\n",
      "\n",
      "[sst2][r=8] Training completed! best_val_acc=0.9402, best_val_f1=0.9470, avg_time/epoch=73.62s, best_epoch=3, early_stopped=True, peak_gpu_memory=1838.29MB, trainable_params=25109762, trainable_ratio=37.1344%\n",
      "[sst2][r=8] Warning: Best model not found, using current model\n",
      "[sst2][r=8] Final test accuracy: 0.9353, test_f1=0.9427\n",
      "\n",
      "------------------------------\n",
      "HiRA with rank r=16\n",
      "------------------------------\n",
      "[sst2][r=16] Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied HiRA to: distilbert.transformer.layer.0.attention.q_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.0.attention.k_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.0.attention.v_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.0.attention.out_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.0.ffn.lin1\n",
      "Applied HiRA to: distilbert.transformer.layer.0.ffn.lin2\n",
      "Applied HiRA to: distilbert.transformer.layer.1.attention.q_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.1.attention.k_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.1.attention.v_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.1.attention.out_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.1.ffn.lin1\n",
      "Applied HiRA to: distilbert.transformer.layer.1.ffn.lin2\n",
      "Applied HiRA to: distilbert.transformer.layer.2.attention.q_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.2.attention.k_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.2.attention.v_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.2.attention.out_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.2.ffn.lin1\n",
      "Applied HiRA to: distilbert.transformer.layer.2.ffn.lin2\n",
      "Applied HiRA to: distilbert.transformer.layer.3.attention.q_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.3.attention.k_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.3.attention.v_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.3.attention.out_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.3.ffn.lin1\n",
      "Applied HiRA to: distilbert.transformer.layer.3.ffn.lin2\n",
      "Applied HiRA to: distilbert.transformer.layer.4.attention.q_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.4.attention.k_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.4.attention.v_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.4.attention.out_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.4.ffn.lin1\n",
      "Applied HiRA to: distilbert.transformer.layer.4.ffn.lin2\n",
      "Applied HiRA to: distilbert.transformer.layer.5.attention.q_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.5.attention.k_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.5.attention.v_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.5.attention.out_lin\n",
      "Applied HiRA to: distilbert.transformer.layer.5.ffn.lin1\n",
      "Applied HiRA to: distilbert.transformer.layer.5.ffn.lin2\n",
      "[sst2][r=16] Trainable params: 25,773,314 || All params: 68,282,114 || Trainable%: 37.7453%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sst2][r=16] Epoch 1 | train_loss=0.2506 | val_acc=0.9342 | val_f1=0.9422 | time=73.64s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sst2][r=16] Epoch 2 | train_loss=0.1051 | val_acc=0.9376 | val_f1=0.9439 | time=73.97s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sst2][r=16] Epoch 3 | train_loss=0.0593 | val_acc=0.9366 | val_f1=0.9433 | time=74.54s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sst2][r=16] Epoch 4 | train_loss=0.0369 | val_acc=0.9345 | val_f1=0.9413 | time=73.63s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sst2][r=16] Epoch 5 | train_loss=0.0256 | val_acc=0.9354 | val_f1=0.9424 | time=73.55s\n",
      "\n",
      "[Early Stopping] No improvement over best F1 for 3 consecutive epochs. Stopped at epoch 5. Best F1: 0.9439 at epoch 2\n",
      "\n",
      "[sst2][r=16] Training completed! best_val_acc=0.9376, best_val_f1=0.9439, avg_time/epoch=73.86s, best_epoch=2, early_stopped=True, peak_gpu_memory=2296.67MB, trainable_params=25773314, trainable_ratio=37.7453%\n",
      "[sst2][r=16] Warning: Best model not found, using current model\n",
      "[sst2][r=16] Final test accuracy: 0.9371, test_f1=0.9448\n",
      "\n",
      "================================================================================\n",
      "BENCHMARK SUMMARY\n",
      "================================================================================\n",
      " Rank Test Acc Test F1 Val Acc Val F1 Best Val F1  Best F1 Epoch Trainable Params Trainable % Total Time (s) Avg Epoch (s) Early Stopped Peak GPU (MB) Sparsity (<1e-3)\n",
      "    2   0.9363  0.9439  0.9379 0.9444      0.9444              3       24,612,098      36.67%         437.71         72.95           Yes       1384.98            1.71%\n",
      "    4   0.9350  0.9430  0.9376 0.9445      0.9445              2       24,777,986      36.82%         366.27         73.25           Yes       1835.24            1.72%\n",
      "    8   0.9353  0.9427  0.9402 0.9470      0.9470              3       25,109,762      37.13%         441.73         73.62           Yes       1838.29            1.71%\n",
      "   16   0.9371  0.9448  0.9376 0.9439      0.9439              2       25,773,314      37.75%         369.32         73.86           Yes       2296.67            1.72%\n",
      "\n",
      "✓ Summary saved to 'HiRA_SST2_benchmark_summary.csv'\n",
      "================================================================================\n",
      "\n",
      "Summary over ranks:\n",
      "Rank=2, Test Acc=0.9363, Test F1=0.9439, Val Acc=0.9379, Val F1=0.9444, Best Val F1=0.9444, Best F1 Epoch=3, Trainable Params=24,612,098, Trainable %=36.67%, Total Time (s)=437.71, Avg Epoch (s)=72.95, Early Stopped=Yes, Peak GPU (MB)=1384.98, Sparsity (<1e-3)=1.71%\n",
      "Rank=4, Test Acc=0.9350, Test F1=0.9430, Val Acc=0.9376, Val F1=0.9445, Best Val F1=0.9445, Best F1 Epoch=2, Trainable Params=24,777,986, Trainable %=36.82%, Total Time (s)=366.27, Avg Epoch (s)=73.25, Early Stopped=Yes, Peak GPU (MB)=1835.24, Sparsity (<1e-3)=1.72%\n",
      "Rank=8, Test Acc=0.9353, Test F1=0.9427, Val Acc=0.9402, Val F1=0.9470, Best Val F1=0.9470, Best F1 Epoch=3, Trainable Params=25,109,762, Trainable %=37.13%, Total Time (s)=441.73, Avg Epoch (s)=73.62, Early Stopped=Yes, Peak GPU (MB)=1838.29, Sparsity (<1e-3)=1.71%\n",
      "Rank=16, Test Acc=0.9371, Test F1=0.9448, Val Acc=0.9376, Val F1=0.9439, Best Val F1=0.9439, Best F1 Epoch=2, Trainable Params=25,773,314, Trainable %=37.75%, Total Time (s)=369.32, Avg Epoch (s)=73.86, Early Stopped=Yes, Peak GPU (MB)=2296.67, Sparsity (<1e-3)=1.72%\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"Training HiRA on SST-2 with different ranks\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "results = []\n",
    "for r in [2, 4, 8, 16]:\n",
    "    print(\"\\n\" + \"-\" * 30)\n",
    "    print(f\"HiRA with rank r={r}\")\n",
    "    print(\"-\" * 30)\n",
    "    res = train_hira_model(\n",
    "        dataset_name=\"sst2\",\n",
    "        model_name=\"distilbert-base-uncased\",\n",
    "        r=r,\n",
    "        lora_alpha=32,\n",
    "        num_epochs=30,        \n",
    "        batch_size=32,\n",
    "        learning_rate=5e-4,\n",
    "        weight_decay=0.01,\n",
    "        warmup_steps=100,\n",
    "        logging_steps=100,\n",
    "        max_length=128,\n",
    "        early_stop_patience=3,\n",
    "        output_dir=\"./results_hira\",\n",
    "        resume_from_checkpoint=False,\n",
    "    )\n",
    "    results.append(res)\n",
    "\n",
    "# Generate and save summary table\n",
    "generate_summary_table(results, save_prefix=\"HiRA_SST2\")\n",
    "\n",
    "print(\"\\nSummary over ranks:\")\n",
    "for res in results:\n",
    "    print(\n",
    "        f\"Rank={res['rank']}, \"\n",
    "        f\"Test Acc={res['final_test_accuracy']:.4f}, \"\n",
    "        f\"Test F1={res['final_test_f1']:.4f}, \"\n",
    "        f\"Val Acc={res['final_val_accuracy']:.4f}, \"\n",
    "        f\"Val F1={res['final_val_f1']:.4f}, \"\n",
    "        f\"Best Val F1={res['best_val_f1']:.4f}, \"\n",
    "        f\"Best F1 Epoch={res['best_f1_epoch']}, \"\n",
    "        f\"Trainable Params={res['trainable_parameters']:,}, \"\n",
    "        f\"Trainable %={res['trainable_percentage']:.2f}%, \"\n",
    "        f\"Total Time (s)={res['total_training_time']:.2f}, \"\n",
    "        f\"Avg Epoch (s)={res['average_epoch_time']:.2f}, \"\n",
    "        f\"Early Stopped={'Yes' if res['early_stopped'] else 'No'}, \"\n",
    "        f\"Peak GPU (MB)={res['peak_gpu_memory_mb']:.2f}, \"\n",
    "        f\"Sparsity (<1e-3)={res['sparsity']*100:.2f}%\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "879aeb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "BENCHMARK SUMMARY\n",
      "================================================================================\n",
      " Rank Test Acc Test F1 Val Acc Val F1 Best Val F1  Best F1 Epoch Trainable Params Trainable % Total Time (s) Avg Epoch (s) Early Stopped Peak GPU (MB) Sparsity (<1e-3)\n",
      "    2   0.9363  0.9439  0.9379 0.9444      0.9444              3       24,612,098      36.67%         437.71         72.95           Yes       1384.98            1.71%\n",
      "    4   0.9350  0.9430  0.9376 0.9445      0.9445              2       24,777,986      36.82%         366.27         73.25           Yes       1835.24            1.72%\n",
      "    8   0.9353  0.9427  0.9402 0.9470      0.9470              3       25,109,762      37.13%         441.73         73.62           Yes       1838.29            1.71%\n",
      "   16   0.9371  0.9448  0.9376 0.9439      0.9439              2       25,773,314      37.75%         369.32         73.86           Yes       2296.67            1.72%\n",
      "\n",
      "✓ Summary saved to 'HiRA_SST2_benchmark_summary.csv'\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Test Acc</th>\n",
       "      <th>Test F1</th>\n",
       "      <th>Val Acc</th>\n",
       "      <th>Val F1</th>\n",
       "      <th>Best Val F1</th>\n",
       "      <th>Best F1 Epoch</th>\n",
       "      <th>Trainable Params</th>\n",
       "      <th>Trainable %</th>\n",
       "      <th>Total Time (s)</th>\n",
       "      <th>Avg Epoch (s)</th>\n",
       "      <th>Early Stopped</th>\n",
       "      <th>Peak GPU (MB)</th>\n",
       "      <th>Sparsity (&lt;1e-3)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.9363</td>\n",
       "      <td>0.9439</td>\n",
       "      <td>0.9379</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>3</td>\n",
       "      <td>24,612,098</td>\n",
       "      <td>36.67%</td>\n",
       "      <td>437.71</td>\n",
       "      <td>72.95</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1384.98</td>\n",
       "      <td>1.71%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.9350</td>\n",
       "      <td>0.9430</td>\n",
       "      <td>0.9376</td>\n",
       "      <td>0.9445</td>\n",
       "      <td>0.9445</td>\n",
       "      <td>2</td>\n",
       "      <td>24,777,986</td>\n",
       "      <td>36.82%</td>\n",
       "      <td>366.27</td>\n",
       "      <td>73.25</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1835.24</td>\n",
       "      <td>1.72%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>0.9353</td>\n",
       "      <td>0.9427</td>\n",
       "      <td>0.9402</td>\n",
       "      <td>0.9470</td>\n",
       "      <td>0.9470</td>\n",
       "      <td>3</td>\n",
       "      <td>25,109,762</td>\n",
       "      <td>37.13%</td>\n",
       "      <td>441.73</td>\n",
       "      <td>73.62</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1838.29</td>\n",
       "      <td>1.71%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>0.9371</td>\n",
       "      <td>0.9448</td>\n",
       "      <td>0.9376</td>\n",
       "      <td>0.9439</td>\n",
       "      <td>0.9439</td>\n",
       "      <td>2</td>\n",
       "      <td>25,773,314</td>\n",
       "      <td>37.75%</td>\n",
       "      <td>369.32</td>\n",
       "      <td>73.86</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2296.67</td>\n",
       "      <td>1.72%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank Test Acc Test F1 Val Acc  Val F1 Best Val F1  Best F1 Epoch  \\\n",
       "0     2   0.9363  0.9439  0.9379  0.9444      0.9444              3   \n",
       "1     4   0.9350  0.9430  0.9376  0.9445      0.9445              2   \n",
       "2     8   0.9353  0.9427  0.9402  0.9470      0.9470              3   \n",
       "3    16   0.9371  0.9448  0.9376  0.9439      0.9439              2   \n",
       "\n",
       "  Trainable Params Trainable % Total Time (s) Avg Epoch (s) Early Stopped  \\\n",
       "0       24,612,098      36.67%         437.71         72.95           Yes   \n",
       "1       24,777,986      36.82%         366.27         73.25           Yes   \n",
       "2       25,109,762      37.13%         441.73         73.62           Yes   \n",
       "3       25,773,314      37.75%         369.32         73.86           Yes   \n",
       "\n",
       "  Peak GPU (MB) Sparsity (<1e-3)  \n",
       "0       1384.98            1.71%  \n",
       "1       1835.24            1.72%  \n",
       "2       1838.29            1.71%  \n",
       "3       2296.67            1.72%  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-generate summary table if needed\n",
    "generate_summary_table(results, save_prefix=\"HiRA_SST2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ac2fa4-7aa3-40d5-9736-7594b0c91ce9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece685",
   "language": "python",
   "name": "ece685"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
