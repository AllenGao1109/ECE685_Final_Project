{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "741d3029-9f9d-4c04-b774-cfeaac57a225",
   "metadata": {},
   "source": [
    "## Group Project LLM (IMDB)\n",
    "\n",
    "- r=2,4,8,16, epoch=30\n",
    "- seed=42\n",
    "- evaluation:\n",
    "    - accuracy, f1, precision, recall\n",
    "    - efficiency (time, trainable parameters, trainable paramters ratio, early stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36ff6988-d8be-4cad-893b-1c4e81d9d60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"default\", module=\"__main__\")\n",
    "warnings.filterwarnings(\"ignore\", module=\".*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a10aac0-477a-4871-a5cf-5ddc8633c849",
   "metadata": {},
   "source": [
    "## Base Model: DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fec7d1d-23c4-46d4-9c02-0ab56c0974e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPU Info] 2 GPU(s) available\n",
      "  GPU 0: NVIDIA RTX 5000 Ada Generation\n",
      "  GPU 1: NVIDIA RTX 5000 Ada Generation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# -------- Model --------\\nmodel = DistilBertForSequenceClassification.from_pretrained(\\n    \"distilbert-base-uncased\", num_labels=2\\n).to(DEVICE)\\n\\ntotal_params = sum(p.numel() for p in model.parameters())\\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\\nratio = trainable_params / total_params\\n\\nprint(f\"Baseline: total={total_params}, trainable={trainable_params}, ratio={ratio:.4%}\")\\n\\n# -------- Train --------\\nargs = TrainingArguments(\\n    output_dir=\"./baseline_distilbert_imdb\",\\n    num_train_epochs=NUM_EPOCHS,\\n    per_device_train_batch_size=BATCH_SIZE,\\n    per_device_eval_batch_size=BATCH_SIZE,\\n    learning_rate=LR,\\n    eval_strategy=\"epoch\",\\n    save_strategy=\"epoch\",\\n    logging_strategy=\"epoch\",\\n    load_best_model_at_end=True,\\n    metric_for_best_model=\"f1\",\\n    greater_is_better=True,\\n    seed=SEED,\\n    report_to=\"none\",\\n)\\n\\ntrainer = Trainer(\\n    model=model,\\n    args=args,\\n    train_dataset=train_ds,\\n    eval_dataset=val_ds,\\n    data_collator=collator,\\n    tokenizer=tokenizer,\\n    compute_metrics=compute_metrics,\\n)\\n\\nstart = time.time()\\ntrainer.train()\\nend = time.time()\\n\\nprint(f\"Baseline training time: {end-start:.2f}s\")\\nprint(\"Eval:\", trainer.evaluate(test_ds))\\nprint(\"Training history:\", trainer.state.log_history)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ================== BASELINE DISTILBERT ================\n",
    "\n",
    "import os, time, random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    DistilBertTokenizerFast,\n",
    "    DistilBertForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    set_seed\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "# Force to use only GPU 1\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# Print GPU info\n",
    "if torch.cuda.is_available():\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"[GPU Info] {num_gpus} GPU(s) available\")\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"[GPU Info] No GPU available, using CPU\")\n",
    "\n",
    "SEED = 42\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DATASET = \"imdb\"\n",
    "TEXT_COL = \"text\"\n",
    "LABEL_COL = \"label\"\n",
    "NUM_EPOCHS = 30\n",
    "BATCH_SIZE = 16\n",
    "LR = 5e-4\n",
    "\n",
    "\n",
    "def set_all_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    set_seed(seed)\n",
    "\n",
    "set_all_seeds(SEED)\n",
    "\n",
    "# -------- Load dataset and split (8:1:1) --------\n",
    "raw = load_dataset(DATASET)\n",
    "train_full = raw[\"train\"]\n",
    "\n",
    "train_temp = train_full.train_test_split(test_size=0.2, seed=SEED)\n",
    "train_ds = train_temp[\"train\"]\n",
    "temp = train_temp[\"test\"]\n",
    "\n",
    "val_test = temp.train_test_split(test_size=0.5, seed=SEED)\n",
    "val_ds = val_test[\"train\"]\n",
    "test_ds = val_test[\"test\"]\n",
    "\n",
    "\n",
    "# -------- Tokenization --------\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "def preprocess(x):\n",
    "    return tokenizer(x[TEXT_COL], truncation=True, max_length=256)\n",
    "\n",
    "train_ds = train_ds.map(preprocess, batched=True)\n",
    "val_ds   = val_ds.map(preprocess, batched=True)\n",
    "test_ds  = test_ds.map(preprocess, batched=True)\n",
    "\n",
    "train_ds = train_ds.rename_column(LABEL_COL, \"labels\")\n",
    "val_ds   = val_ds.rename_column(LABEL_COL, \"labels\")\n",
    "test_ds  = test_ds.rename_column(LABEL_COL, \"labels\")\n",
    "\n",
    "cols = [\"input_ids\",\"attention_mask\",\"labels\"]\n",
    "train_ds.set_format(type=\"torch\", columns=cols)\n",
    "val_ds.set_format(type=\"torch\", columns=cols)\n",
    "test_ds.set_format(type=\"torch\", columns=cols)\n",
    "\n",
    "collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# -------- Metrics --------\n",
    "acc = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "prec = evaluate.load(\"precision\")\n",
    "rec = evaluate.load(\"recall\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": acc.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "        \"f1\": f1.compute(predictions=preds, references=labels, average=\"binary\")[\"f1\"],\n",
    "        \"precision\": prec.compute(predictions=preds, references=labels, average=\"binary\")[\"precision\"],\n",
    "        \"recall\": rec.compute(predictions=preds, references=labels, average=\"binary\")[\"recall\"],\n",
    "    }\n",
    "\n",
    "'''\n",
    "# -------- Model --------\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2\n",
    ").to(DEVICE)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "ratio = trainable_params / total_params\n",
    "\n",
    "print(f\"Baseline: total={total_params}, trainable={trainable_params}, ratio={ratio:.4%}\")\n",
    "\n",
    "# -------- Train --------\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./baseline_distilbert_imdb\",\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LR,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    seed=SEED,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "trainer.train()\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Baseline training time: {end-start:.2f}s\")\n",
    "print(\"Eval:\", trainer.evaluate(test_ds))\n",
    "print(\"Training history:\", trainer.state.log_history)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1aa51c2-0a97-430e-9ef3-e8e0649673ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport pandas as pd\\n# -------- Final Evaluation --------\\nfinal_metrics = trainer.evaluate(test_ds)\\n\\n# -------- Save metrics --------\\nos.makedirs(\"./baseline_distilbert_imdb\", exist_ok=True)\\nwith open(\"./baseline_distilbert_imdb/final_metrics.json\", \"w\") as f:\\n    json.dump(final_metrics, f, indent=4)\\n\\nprint(\"Saved final metrics to baseline_distilbert_imdb/final_metrics.json\")\\n\\n# -------- Save model --------\\ntrainer.save_model(\"./baseline_distilbert_imdb/final_model\")\\nprint(\"Saved model to baseline_distilbert_imdb/final_model\")\\n\\n# -------- Training history --------\\nlog_history = trainer.state.log_history\\ndf_logs = pd.DataFrame(trainer.state.log_history)\\n# Separate clean tables\\ndf_train = df_logs[df_logs[\"loss\"].notnull()].reset_index(drop=True)\\ndf_eval  = df_logs[df_logs[\"eval_loss\"].notnull()].reset_index(drop=True)\\n\\ndf_train.to_csv(\"./baseline_distilbert_imdb/train_log.csv\", index=False)\\ndf_eval.to_csv(\"./baseline_distilbert_imdb/eval_log.csv\", index=False)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import pandas as pd\n",
    "# -------- Final Evaluation --------\n",
    "final_metrics = trainer.evaluate(test_ds)\n",
    "\n",
    "# -------- Save metrics --------\n",
    "os.makedirs(\"./baseline_distilbert_imdb\", exist_ok=True)\n",
    "with open(\"./baseline_distilbert_imdb/final_metrics.json\", \"w\") as f:\n",
    "    json.dump(final_metrics, f, indent=4)\n",
    "\n",
    "print(\"Saved final metrics to baseline_distilbert_imdb/final_metrics.json\")\n",
    "\n",
    "# -------- Save model --------\n",
    "trainer.save_model(\"./baseline_distilbert_imdb/final_model\")\n",
    "print(\"Saved model to baseline_distilbert_imdb/final_model\")\n",
    "\n",
    "# -------- Training history --------\n",
    "log_history = trainer.state.log_history\n",
    "df_logs = pd.DataFrame(trainer.state.log_history)\n",
    "# Separate clean tables\n",
    "df_train = df_logs[df_logs[\"loss\"].notnull()].reset_index(drop=True)\n",
    "df_eval  = df_logs[df_logs[\"eval_loss\"].notnull()].reset_index(drop=True)\n",
    "\n",
    "df_train.to_csv(\"./baseline_distilbert_imdb/train_log.csv\", index=False)\n",
    "df_eval.to_csv(\"./baseline_distilbert_imdb/eval_log.csv\", index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91335c9-b594-4f54-b5bb-e19456ea0347",
   "metadata": {},
   "source": [
    "## Sparse LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93887a12-1f4b-4390-b867-0e9c388afb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Training Sparse LoRA DistilBERT with rank = 2, epochs = 30\n",
      "================================================================================\n",
      "[Rank 2] total params: 67,620,868\n",
      "[Rank 2] trainable params: 665,858\n",
      "[Rank 2] trainable params ratio (trainable / total): 0.9847%\n",
      "[Rank 2] total params: 67,620,868\n",
      "[Rank 2] trainable params: 665,858\n",
      "[Rank 2] trainable params ratio (trainable / total): 0.9847%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6250' max='18750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 6250/18750 12:32 < 25:04, 8.31 it/s, Epoch 10/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.263800</td>\n",
       "      <td>0.260634</td>\n",
       "      <td>0.889200</td>\n",
       "      <td>0.891415</td>\n",
       "      <td>0.886204</td>\n",
       "      <td>0.896688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.257400</td>\n",
       "      <td>0.249981</td>\n",
       "      <td>0.901200</td>\n",
       "      <td>0.902871</td>\n",
       "      <td>0.900392</td>\n",
       "      <td>0.905363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.242400</td>\n",
       "      <td>0.264484</td>\n",
       "      <td>0.904800</td>\n",
       "      <td>0.905104</td>\n",
       "      <td>0.915323</td>\n",
       "      <td>0.895110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.214100</td>\n",
       "      <td>0.256576</td>\n",
       "      <td>0.907600</td>\n",
       "      <td>0.908732</td>\n",
       "      <td>0.910530</td>\n",
       "      <td>0.906940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.201500</td>\n",
       "      <td>0.266671</td>\n",
       "      <td>0.906800</td>\n",
       "      <td>0.907134</td>\n",
       "      <td>0.917002</td>\n",
       "      <td>0.897476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.189300</td>\n",
       "      <td>0.269039</td>\n",
       "      <td>0.911600</td>\n",
       "      <td>0.912683</td>\n",
       "      <td>0.914489</td>\n",
       "      <td>0.910883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.181700</td>\n",
       "      <td>0.294015</td>\n",
       "      <td>0.910800</td>\n",
       "      <td>0.914592</td>\n",
       "      <td>0.889054</td>\n",
       "      <td>0.941640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.158700</td>\n",
       "      <td>0.305144</td>\n",
       "      <td>0.906400</td>\n",
       "      <td>0.906699</td>\n",
       "      <td>0.916935</td>\n",
       "      <td>0.896688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.152400</td>\n",
       "      <td>0.311745</td>\n",
       "      <td>0.904000</td>\n",
       "      <td>0.904913</td>\n",
       "      <td>0.909236</td>\n",
       "      <td>0.900631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.149100</td>\n",
       "      <td>0.345914</td>\n",
       "      <td>0.908800</td>\n",
       "      <td>0.908581</td>\n",
       "      <td>0.924144</td>\n",
       "      <td>0.893533</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Early Stopping] No improvement over best F1 for 3 consecutive epochs. Stopping training at epoch 10.\n",
      "[Early Stopping Info] Best F1: 0.9146 at epoch 7\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='158' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [79/79 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Results for rank = 2:\n",
      "  Test Accuracy: 0.8956\n",
      "  Test F1: 0.8986\n",
      "  Validation Accuracy: 0.9108\n",
      "  Validation F1: 0.9146\n",
      "  Total Training Time: 753.29s\n",
      "  Average Epoch Time: 70.57s\n",
      "  Best F1 Epoch: 7\n",
      "  Best Val F1: 0.9146\n",
      "  Early Stopped: True\n",
      "  Stop Info: epochs_without_improvement=3\n",
      "  Peak GPU Memory: 1476.78 MB\n",
      "  LoRA Sparsity (<1e-3): 3.36%\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rank 2] Saved model to /hpc/group/xielab/hl385/LoRA/sparse_lora_imdb_rank2/final_model\n",
      "[Rank 2] Saved log history to log_history.json\n",
      "\n",
      "================================================================================\n",
      "Training Sparse LoRA DistilBERT with rank = 4, epochs = 30\n",
      "================================================================================\n",
      "[Rank 4] total params: 67,694,596\n",
      "[Rank 4] trainable params: 739,586\n",
      "[Rank 4] trainable params ratio (trainable / total): 1.0925%\n",
      "[Rank 4] total params: 67,694,596\n",
      "[Rank 4] trainable params: 739,586\n",
      "[Rank 4] trainable params ratio (trainable / total): 1.0925%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14' max='18750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   14/18750 00:01 < 32:29, 9.61 it/s, Epoch 0.02/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 206\u001b[39m\n\u001b[32m    193\u001b[39m trainer = SparseLoraTrainer(\n\u001b[32m    194\u001b[39m     l1_lambda=L1_LAMBDA,\n\u001b[32m    195\u001b[39m     model=lora_model,\n\u001b[32m   (...)\u001b[39m\u001b[32m    202\u001b[39m     callbacks=[metrics_callback],\n\u001b[32m    203\u001b[39m )\n\u001b[32m    205\u001b[39m start_time = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    207\u001b[39m end_time = time.time()\n\u001b[32m    208\u001b[39m total_train_time = end_time - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/xielab/hl385/miniconda3/envs/ece685/lib/python3.12/site-packages/transformers/trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/xielab/hl385/miniconda3/envs/ece685/lib/python3.12/site-packages/transformers/trainer.py:2679\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m   2674\u001b[39m     tr_loss_step = \u001b[38;5;28mself\u001b[39m.training_step(model, inputs, num_items_in_batch)\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m-> \u001b[39m\u001b[32m2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n\u001b[32m   2683\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ================== SPARSE LoRA MODEL =================\n",
    "\n",
    "from typing import Dict, Any, List, Optional\n",
    "import math\n",
    "import json\n",
    "import pandas as pd\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# -------- Sparse LoRA config --------\n",
    "CHECKPOINT_DIR = \"/hpc/group/xielab/hl385/LoRA\"  # Directory to save checkpoints\n",
    "RANKS: List[int] = [2, 4, 8, 16]\n",
    "L1_LAMBDA = 1e-5   # sparsity strength for LoRA weights\n",
    "\n",
    "\n",
    "def count_trainable_params(model: torch.nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def count_total_params(model: torch.nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "\n",
    "def compute_lora_sparsity(model: torch.nn.Module, threshold: float = 1e-3) -> float:\n",
    "    \"\"\"\n",
    "    Approximate sparsity: fraction of LoRA parameters with |w| < threshold.\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    near_zero = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"lora_\" in name and param.requires_grad:\n",
    "            data = param.detach().abs()\n",
    "            total += data.numel()\n",
    "            near_zero += (data < threshold).sum().item()\n",
    "    return near_zero / total if total > 0 else math.nan\n",
    "\n",
    "\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "class MetricsCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    Callback to track metrics and implement early stopping.\n",
    "    \"\"\"\n",
    "    def __init__(self, early_stop_patience=3):\n",
    "        self.epoch_times = []\n",
    "        self.epoch_start_time = None\n",
    "        self.epoch_f1s = []\n",
    "        self.best_f1 = 0.0\n",
    "        self.best_f1_epoch = None\n",
    "        self.logged_epochs = set()\n",
    "        # Early stopping: stop when validation F1 fails to improve over best_f1 for `early_stop_patience` consecutive epochs\n",
    "        self.early_stop_patience = early_stop_patience\n",
    "        self.epochs_without_improvement = 0\n",
    "        self.early_stopped = False\n",
    "\n",
    "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
    "        self.epoch_start_time = time.time()\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        if self.epoch_start_time is not None:\n",
    "            epoch_time = time.time() - self.epoch_start_time\n",
    "            self.epoch_times.append(epoch_time)\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
    "        if 'eval_f1' in metrics and state.epoch > 0:\n",
    "            f1 = metrics['eval_f1']\n",
    "            current_epoch = int(state.epoch)\n",
    "            \n",
    "            if current_epoch not in self.logged_epochs:\n",
    "                self.logged_epochs.add(current_epoch)\n",
    "                self.epoch_f1s.append(f1)\n",
    "\n",
    "                # Update best F1 and reset counter if improved\n",
    "                if f1 > self.best_f1:\n",
    "                    self.best_f1 = f1\n",
    "                    self.best_f1_epoch = current_epoch\n",
    "                    self.epochs_without_improvement = 0\n",
    "                else:\n",
    "                    # F1 did not improve over best_f1\n",
    "                    self.epochs_without_improvement += 1\n",
    "\n",
    "                # Trigger early stopping if no improvement for patience epochs\n",
    "                if self.epochs_without_improvement >= self.early_stop_patience and not self.early_stopped:\n",
    "                    self.early_stopped = True\n",
    "                    # Request Trainer to stop training after this evaluation\n",
    "                    control.should_training_stop = True\n",
    "                    print(f\"\\n[Early Stopping] No improvement over best F1 for {self.epochs_without_improvement} consecutive epochs. Stopping training at epoch {current_epoch}.\")\n",
    "                    print(f\"[Early Stopping Info] Best F1: {self.best_f1:.4f} at epoch {self.best_f1_epoch}\\n\")\n",
    "\n",
    "\n",
    "class SparseLoraTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Trainer with L1 penalty only on LoRA parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self, l1_lambda: float = 0.0, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.l1_lambda = l1_lambda\n",
    "\n",
    "    def compute_loss(\n",
    "        self,\n",
    "        model,\n",
    "        inputs,\n",
    "        return_outputs: bool = False,\n",
    "        num_items_in_batch: Optional[int] = None,\n",
    "    ):\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        if self.l1_lambda > 0:\n",
    "            l1_reg = 0.0\n",
    "            for name, param in model.named_parameters():\n",
    "                if \"lora_\" in name and param.requires_grad:\n",
    "                    l1_reg = l1_reg + param.abs().sum()\n",
    "            loss = loss + self.l1_lambda * l1_reg\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "results_per_rank: List[Dict[str, Any]] = []\n",
    "\n",
    "for r in RANKS:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"Training Sparse LoRA DistilBERT with rank = {r}, epochs = {NUM_EPOCHS}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    set_all_seeds(SEED)\n",
    "\n",
    "    output_dir = f\"{CHECKPOINT_DIR}/sparse_lora_imdb_rank{r}\"\n",
    "    checkpoint_dir = None\n",
    "    resume_from_checkpoint = False  # Set to True to resume from checkpoint\n",
    "    if resume_from_checkpoint and os.path.exists(output_dir):\n",
    "        checkpoints = [d for d in os.listdir(output_dir) if d.startswith(\"checkpoint-\")]\n",
    "        if checkpoints:\n",
    "            checkpoints.sort(key=lambda x: int(x.split(\"-\")[1]))\n",
    "            checkpoint_dir = os.path.join(output_dir, checkpoints[-1])\n",
    "            print(f\"Found existing checkpoint: {checkpoint_dir}\")\n",
    "            print(f\"Resuming training from this checkpoint...\\n\")\n",
    "\n",
    "    # Reset GPU memory stats\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    # Base DistilBERT for this rank\n",
    "    base_model = DistilBertForSequenceClassification.from_pretrained(\n",
    "        \"distilbert-base-uncased\",\n",
    "        num_labels=2,\n",
    "    )\n",
    "\n",
    "    # LoRA config: attention projections in DistilBERT\n",
    "    lora_config = LoraConfig(\n",
    "        r=r,\n",
    "        lora_alpha=2 * r,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=\"SEQ_CLS\",    # sequence classification\n",
    "        target_modules=[\"q_lin\", \"k_lin\", \"v_lin\", \"out_lin\"],\n",
    "    )\n",
    "\n",
    "    lora_model = get_peft_model(base_model, lora_config)\n",
    "    lora_model.to(DEVICE)\n",
    "\n",
    "    total_params = count_total_params(lora_model)\n",
    "    trainable_params = count_trainable_params(lora_model)\n",
    "    param_ratio = trainable_params / total_params\n",
    "\n",
    "    print(f\"[Rank {r}] total params: {total_params:,}\")\n",
    "    print(f\"[Rank {r}] trainable params: {trainable_params:,}\")\n",
    "    print(f\"[Rank {r}] trainable params ratio (trainable / total): {param_ratio:.4%}\")\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    training_args_lora = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        learning_rate=LR,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=100,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        greater_is_better=True,\n",
    "        save_total_limit=2,\n",
    "        seed=SEED,\n",
    "        report_to=\"none\",\n",
    "        disable_tqdm=False,\n",
    "    )\n",
    "\n",
    "    # Create metrics callback\n",
    "    metrics_callback = MetricsCallback(early_stop_patience=3)\n",
    "\n",
    "    trainer = SparseLoraTrainer(\n",
    "        l1_lambda=L1_LAMBDA,\n",
    "        model=lora_model,\n",
    "        args=training_args_lora,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[metrics_callback],\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "    trainer.train(resume_from_checkpoint=checkpoint_dir if checkpoint_dir else None)\n",
    "    end_time = time.time()\n",
    "    total_train_time = end_time - start_time\n",
    "    \n",
    "    # Calculate metrics\n",
    "    avg_epoch_time = sum(metrics_callback.epoch_times) / len(metrics_callback.epoch_times) if metrics_callback.epoch_times else 0.0\n",
    "    \n",
    "    # Get peak GPU memory\n",
    "    peak_gpu_memory_mb = 0.0\n",
    "    if torch.cuda.is_available():\n",
    "        peak_gpu_memory_mb = torch.cuda.max_memory_allocated() / (1024 ** 2)\n",
    "\n",
    "    # --- final evals (using best model loaded by load_best_model_at_end=True) ---\n",
    "    val_metrics = trainer.evaluate(eval_dataset=val_ds)\n",
    "    test_metrics = trainer.evaluate(eval_dataset=test_ds)\n",
    "    lora_sparsity = compute_lora_sparsity(lora_model, threshold=1e-3)\n",
    "    \n",
    "    # Print results in summary table format\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Results for rank = {r}:\")\n",
    "    print(f\"  Test Accuracy: {test_metrics.get('eval_accuracy', 0.0):.4f}\")\n",
    "    print(f\"  Test F1: {test_metrics.get('eval_f1', 0.0):.4f}\")\n",
    "    print(f\"  Validation Accuracy: {val_metrics.get('eval_accuracy', 0.0):.4f}\")\n",
    "    print(f\"  Validation F1: {val_metrics.get('eval_f1', 0.0):.4f}\")\n",
    "    print(f\"  Total Training Time: {total_train_time:.2f}s\")\n",
    "    print(f\"  Average Epoch Time: {avg_epoch_time:.2f}s\")\n",
    "    print(f\"  Best F1 Epoch: {metrics_callback.best_f1_epoch}\")\n",
    "    print(f\"  Best Val F1: {metrics_callback.best_f1:.4f}\")\n",
    "    print(f\"  Early Stopped: {metrics_callback.early_stopped}\")\n",
    "    if metrics_callback.early_stopped:\n",
    "        print(f\"  Stop Info: epochs_without_improvement={metrics_callback.epochs_without_improvement}\")\n",
    "    print(f\"  Peak GPU Memory: {peak_gpu_memory_mb:.2f} MB\")\n",
    "    print(f\"  LoRA Sparsity (<1e-3): {lora_sparsity:.2%}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "    # ==========================\n",
    "    # SAVE METRICS / MODEL / LOG\n",
    "    # ==========================\n",
    "    # 1) save metrics\n",
    "    metrics_payload = {\n",
    "        \"rank\": r,\n",
    "        \"total_train_time_sec\": total_train_time,\n",
    "        \"avg_epoch_time_sec\": avg_epoch_time,\n",
    "        \"best_f1_epoch\": metrics_callback.best_f1_epoch,\n",
    "        \"best_val_f1\": float(metrics_callback.best_f1),\n",
    "        \"early_stopped\": metrics_callback.early_stopped,\n",
    "        \"peak_gpu_memory_mb\": peak_gpu_memory_mb,\n",
    "        \"total_params\": int(total_params),\n",
    "        \"trainable_params\": int(trainable_params),\n",
    "        \"param_ratio\": float(param_ratio),\n",
    "        \"lora_sparsity_<1e-3\": float(lora_sparsity),\n",
    "        \"val_metrics\": val_metrics,\n",
    "        \"test_metrics\": test_metrics,\n",
    "    }\n",
    "    with open(os.path.join(output_dir, \"final_metrics.json\"), \"w\") as f:\n",
    "        json.dump(metrics_payload, f, indent=4)\n",
    "\n",
    "    # 2) save final model (best checkpoint)\n",
    "    final_model_dir = os.path.join(output_dir, \"final_model\")\n",
    "    trainer.save_model(final_model_dir)  # saves model + config\n",
    "    tokenizer.save_pretrained(final_model_dir)  # save tokenizer too\n",
    "    print(f\"[Rank {r}] Saved model to {final_model_dir}\")\n",
    "\n",
    "    # 3) save training history\n",
    "    log_history = trainer.state.log_history\n",
    "    with open(os.path.join(output_dir, \"log_history.json\"), \"w\") as f:\n",
    "        json.dump(log_history, f, indent=4)\n",
    "    print(f\"[Rank {r}] Saved log history to log_history.json\")\n",
    "\n",
    "    # --- store in-memory summary for printing ---\n",
    "    results_per_rank.append(\n",
    "        {\n",
    "            \"rank\": r,\n",
    "            \"final_test_accuracy\": test_metrics.get('eval_accuracy', 0.0),\n",
    "            \"final_test_f1\": test_metrics.get('eval_f1', 0.0),\n",
    "            \"final_val_accuracy\": val_metrics.get('eval_accuracy', 0.0),\n",
    "            \"final_val_f1\": val_metrics.get('eval_f1', 0.0),\n",
    "            \"total_parameters\": total_params,\n",
    "            \"trainable_parameters\": trainable_params,\n",
    "            \"trainable_percentage\": param_ratio * 100,\n",
    "            \"total_training_time\": total_train_time,\n",
    "            \"average_epoch_time\": avg_epoch_time,\n",
    "            \"peak_gpu_memory_mb\": peak_gpu_memory_mb,\n",
    "            \"sparsity\": lora_sparsity,\n",
    "            \"best_f1_epoch\": metrics_callback.best_f1_epoch,\n",
    "            \"best_val_f1\": metrics_callback.best_f1,\n",
    "            \"early_stopped\": metrics_callback.early_stopped,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Generate and save summary table\n",
    "print(\"\\n\\n\" + \"=\" * 80)\n",
    "print(\"GENERATING SUMMARY TABLE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "summary_data = []\n",
    "for result in results_per_rank:\n",
    "    summary_data.append({\n",
    "        \"Rank\": result['rank'],\n",
    "        \"Test Acc\": f\"{result['final_test_accuracy']:.4f}\",\n",
    "        \"Test F1\": f\"{result['final_test_f1']:.4f}\",\n",
    "        \"Val Acc\": f\"{result['final_val_accuracy']:.4f}\",\n",
    "        \"Val F1\": f\"{result['final_val_f1']:.4f}\",\n",
    "        \"Best Val F1\": f\"{result['best_val_f1']:.4f}\",\n",
    "        \"Best F1 Epoch\": result['best_f1_epoch'],\n",
    "        \"Trainable Params\": f\"{result['trainable_parameters']:,}\",\n",
    "        \"Trainable %\": f\"{result['trainable_percentage']:.2f}%\",\n",
    "        \"Total Time (s)\": f\"{result['total_training_time']:.2f}\",\n",
    "        \"Avg Epoch (s)\": f\"{result['average_epoch_time']:.2f}\",\n",
    "        \"Early Stopped\": \"Yes\" if result['early_stopped'] else \"No\",\n",
    "        \"Peak GPU (MB)\": f\"{result['peak_gpu_memory_mb']:.2f}\",\n",
    "        \"Sparsity (<1e-3)\": f\"{result['sparsity']*100:.2f}%\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BENCHMARK SUMMARY - Sparse LoRA (IMDB)\")\n",
    "print(\"=\"*80)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Save summary to CSV\n",
    "csv_filename = \"SparseLoRA_IMDB_benchmark_summary.csv\"\n",
    "summary_df.to_csv(csv_filename, index=False)\n",
    "print(f\"\\n✓ Summary saved to '{csv_filename}'\")\n",
    "\n",
    "# Save complete results\n",
    "with open(\"sparse_lora_imdb_all_results.json\", \"w\") as f:\n",
    "    json.dump(results_per_rank, f, indent=2)\n",
    "print(\"✓ Complete results saved to 'sparse_lora_imdb_all_results.json'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece685",
   "language": "python",
   "name": "ece685"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
