{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99e08e18-3ddd-4f64-83eb-60528b6eef2e",
   "metadata": {},
   "source": [
    "# Pre-trained Model Evaluation: DistilBERT on IMDB\n",
    "\n",
    "This notebook evaluates the pre-trained DistilBERT model (without fine-tuning) on the IMDB sentiment classification task.\n",
    "\n",
    "**Configuration:**\n",
    "- Base Model: DistilBERT (pre-trained, no fine-tuning)\n",
    "- Dataset: IMDB (Movie Reviews)\n",
    "- Split Ratio: Train:Val:Test = 8:1:1\n",
    "- Random Seed: 42\n",
    "\n",
    "**Metrics Tracked:**\n",
    "- Test Accuracy & F1\n",
    "- Validation Accuracy & F1\n",
    "- Inference time\n",
    "- GPU memory used\n",
    "- Total parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c26b002-cb91-49a0-8930-4b1a5fd07e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/group/xielab/hl385/miniconda3/envs/ece685/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA H200 MIG 1g.18gb\n",
      "Initial GPU Memory: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import warnings\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import gc\n",
    "\n",
    "# Filter out specific warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='torch.nn.parallel._functions')\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Initial GPU Memory: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ede627b7-5331-4ac1-8243-5c1a534c8f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "print(\"Random seed set to 42\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64445228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SST-2 dataset...\n",
      "Dataset split complete:\n",
      "  Train: 53879 samples\n",
      "  Validation: 6734 samples\n",
      "  Test: 6736 samples\n",
      "Dataset split complete:\n",
      "  Train: 53879 samples\n",
      "  Validation: 6734 samples\n",
      "  Test: 6736 samples\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare IMDB dataset\n",
    "print(\"Loading IMDB dataset...\")\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Get full training data\n",
    "train_data = dataset[\"train\"]\n",
    "total_samples = len(train_data)\n",
    "\n",
    "# Calculate split sizes (8:1:1)\n",
    "train_size = int(0.8 * total_samples)\n",
    "val_size = int(0.1 * total_samples)\n",
    "test_size = total_samples - train_size - val_size\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset = train_data.select(range(train_size))\n",
    "val_dataset = train_data.select(range(train_size, train_size + val_size))\n",
    "test_dataset = train_data.select(range(train_size + val_size, total_samples))\n",
    "\n",
    "print(f\"Dataset split complete:\")\n",
    "print(f\"  Train: {len(train_dataset)} samples\")\n",
    "print(f\"  Validation: {len(val_dataset)} samples\")\n",
    "print(f\"  Test: {len(test_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "311d4b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "Tokenizing datasets...\n",
      "Tokenization complete!\n",
      "Tokenizing datasets...\n",
      "Tokenization complete!\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the dataset\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "print(\"Tokenizing datasets...\")\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_val = val_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "print(\"Tokenization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27246a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define compute metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"f1\": f1_score(labels, predictions, average=\"binary\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eeea7161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate pre-trained model (no training)\n",
    "def evaluate_pretrained_model():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Evaluating Pre-trained DistilBERT Model\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Reset seed\n",
    "    set_seed(42)\n",
    "    \n",
    "    # Clear GPU memory\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    # Load pre-trained model (no fine-tuning)\n",
    "    print(\"Loading pre-trained DistilBERT model...\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"distilbert-base-uncased\",\n",
    "        num_labels=2\n",
    "    )\n",
    "    \n",
    "    # Print model parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"\\nModel Parameters:\")\n",
    "    print(f\"  Total parameters: {total_params:,}\\n\")\n",
    "    \n",
    "    # Move model to device\n",
    "    model.to(device)\n",
    "    \n",
    "    # Record GPU memory\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        initial_memory = torch.cuda.memory_allocated() / 1024**2\n",
    "        print(f\"GPU Memory after loading model: {initial_memory:.2f} MB\")\n",
    "    \n",
    "    # Create trainer for evaluation only (no training)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results_pretrained_imdb\",\n",
    "        per_device_eval_batch_size=16,\n",
    "        seed=42,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    print(\"\\nEvaluating on validation set...\")\n",
    "    start_time = time.time()\n",
    "    val_results = trainer.evaluate(tokenized_val)\n",
    "    val_time = time.time() - start_time\n",
    "    val_accuracy = val_results['eval_accuracy']\n",
    "    val_f1 = val_results.get('eval_f1', 0.0)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"Evaluating on test set...\")\n",
    "    start_time = time.time()\n",
    "    test_results = trainer.evaluate(tokenized_test)\n",
    "    test_time = time.time() - start_time\n",
    "    test_accuracy = test_results['eval_accuracy']\n",
    "    test_f1 = test_results.get('eval_f1', 0.0)\n",
    "    \n",
    "    # Get peak GPU memory\n",
    "    if torch.cuda.is_available():\n",
    "        peak_memory = torch.cuda.max_memory_allocated() / 1024**2\n",
    "        print(f\"\\nPeak GPU Memory during evaluation: {peak_memory:.2f} MB\")\n",
    "    else:\n",
    "        peak_memory = 0\n",
    "    \n",
    "    # Compile results\n",
    "    results = {\n",
    "        \"model_type\": \"pre-trained (no fine-tuning)\",\n",
    "        \"test_accuracy\": test_accuracy,\n",
    "        \"test_f1\": test_f1,\n",
    "        \"val_accuracy\": val_accuracy,\n",
    "        \"val_f1\": val_f1,\n",
    "        \"total_parameters\": total_params,\n",
    "        \"test_inference_time\": test_time,\n",
    "        \"val_inference_time\": val_time,\n",
    "        \"peak_gpu_memory_mb\": peak_memory,\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Pre-trained Model Results:\")\n",
    "    print(f\"  Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"  Test F1: {test_f1:.4f}\")\n",
    "    print(f\"  Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"  Validation F1: {val_f1:.4f}\")\n",
    "    print(f\"  Test Inference Time: {test_time:.2f}s\")\n",
    "    print(f\"  Val Inference Time: {val_time:.2f}s\")\n",
    "    print(f\"  Peak GPU Memory: {peak_memory:.2f} MB\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Clean up\n",
    "    del model\n",
    "    del trainer\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return results\n",
    "    print(f\"  Time to Convergence: {time_to_convergence:.2f}s\")\n",
    "    print(f\"  Peak GPU Memory: {peak_memory:.2f} MB\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Clean up\n",
    "    del model\n",
    "    del trainer\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9447e1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Pre-trained DistilBERT Model...\n",
      "Random seed: 42\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Evaluating Pre-trained DistilBERT Model\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained DistilBERT model...\n",
      "\n",
      "Model Parameters:\n",
      "  Total parameters: 66,955,010\n",
      "\n",
      "GPU Memory after loading model: 256.50 MB\n",
      "\n",
      "Evaluating on validation set...\n",
      "GPU Memory after loading model: 256.50 MB\n",
      "\n",
      "Evaluating on validation set...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='422' max='211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [211/211 00:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on test set...\n",
      "\n",
      "Peak GPU Memory during evaluation: 331.82 MB\n",
      "\n",
      "================================================================================\n",
      "Pre-trained Model Results:\n",
      "  Test Accuracy: 0.4210\n",
      "  Test F1: 0.0714\n",
      "  Validation Accuracy: 0.4253\n",
      "  Validation F1: 0.0547\n",
      "  Test Inference Time: 14.14s\n",
      "  Val Inference Time: 14.27s\n",
      "  Peak GPU Memory: 331.82 MB\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Evaluation completed!\n",
      "================================================================================\n",
      "\n",
      "Peak GPU Memory during evaluation: 331.82 MB\n",
      "\n",
      "================================================================================\n",
      "Pre-trained Model Results:\n",
      "  Test Accuracy: 0.4210\n",
      "  Test F1: 0.0714\n",
      "  Validation Accuracy: 0.4253\n",
      "  Validation F1: 0.0547\n",
      "  Test Inference Time: 14.14s\n",
      "  Val Inference Time: 14.27s\n",
      "  Peak GPU Memory: 331.82 MB\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Evaluation completed!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Evaluate pre-trained model (no training/fine-tuning)\n",
    "print(\"Evaluating Pre-trained DistilBERT Model on IMDB...\")\n",
    "print(f\"Random seed: 42\\n\")\n",
    "\n",
    "results = evaluate_pretrained_model()\n",
    "\n",
    "# Save results\n",
    "with open(\"pretrained_imdb_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Evaluation completed!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2ac7379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PRE-TRAINED MODEL SUMMARY\n",
      "================================================================================\n",
      "                  Model Type Test Accuracy Test F1 Val Accuracy Val F1 Total Params Test Inference Time (s) Val Inference Time (s) Peak GPU Memory (MB)\n",
      "pre-trained (no fine-tuning)        0.4210  0.0714       0.4253 0.0547   66,955,010                   14.14                  14.27               331.82\n",
      "\n",
      "✓ Summary saved to 'pretrained_summary.csv'\n",
      "✓ Complete results saved to 'pretrained_results.json'\n"
     ]
    }
   ],
   "source": [
    "# Display results summary\n",
    "import pandas as pd\n",
    "\n",
    "# Create summary\n",
    "summary_data = {\n",
    "    \"Model Type\": results[\"model_type\"],\n",
    "    \"Test Accuracy\": f\"{results['test_accuracy']:.4f}\",\n",
    "    \"Test F1\": f\"{results['test_f1']:.4f}\",\n",
    "    \"Val Accuracy\": f\"{results['val_accuracy']:.4f}\",\n",
    "    \"Val F1\": f\"{results['val_f1']:.4f}\",\n",
    "    \"Total Params\": f\"{results['total_parameters']:,}\",\n",
    "    \"Test Inference Time (s)\": f\"{results['test_inference_time']:.2f}\",\n",
    "    \"Val Inference Time (s)\": f\"{results['val_inference_time']:.2f}\",\n",
    "    \"Peak GPU Memory (MB)\": f\"{results['peak_gpu_memory_mb']:.2f}\"\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame([summary_data])\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PRE-TRAINED MODEL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Save summary to CSV\n",
    "summary_df.to_csv(\"pretrained_imdb_summary.csv\", index=False)\n",
    "print(\"\\n✓ Summary saved to 'pretrained_imdb_summary.csv'\")\n",
    "print(\"✓ Complete results saved to 'pretrained_imdb_results.json'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece685",
   "language": "python",
   "name": "ece685"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
