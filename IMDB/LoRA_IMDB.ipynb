{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99e08e18-3ddd-4f64-83eb-60528b6eef2e",
   "metadata": {},
   "source": [
    "# LoRA Fine-tuning Benchmark: DistilBERT on IMDB\n",
    "\n",
    "This notebook benchmarks LoRA adaptation methods for fine-tuning DistilBERT on the IMDB sentiment classification task.\n",
    "\n",
    "**Configuration:**\n",
    "- Base Model: DistilBERT\n",
    "- Dataset: IMDB (Movie Reviews)\n",
    "- Split Ratio: Train:Val:Test = 8:1:1\n",
    "- LoRA Ranks: [2, 4, 8, 16]\n",
    "- Training Epochs: 10\n",
    "- Random Seed: 42\n",
    "\n",
    "**Metrics Tracked:**\n",
    "- Final Accuracy\n",
    "- Running time per epoch\n",
    "- Total training time\n",
    "- Time to convergence\n",
    "- GPU memory used\n",
    "- Total parameters\n",
    "- Trainable parameters\n",
    "- Convergence epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c26b002-cb91-49a0-8930-4b1a5fd07e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/group/xielab/hl385/miniconda3/envs/ece685/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import warnings\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from sklearn.metrics import accuracy_score\n",
    "import gc\n",
    "\n",
    "# Filter out specific warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='torch.nn.parallel._functions')\n",
    "\n",
    "# Helper function to check if this is the main process (for multi-GPU training)\n",
    "def is_main_process():\n",
    "    # Check LOCAL_RANK environment variable\n",
    "    # In distributed training: rank 0 is main process\n",
    "    # In single process or DataParallel: LOCAL_RANK not set, return True\n",
    "    local_rank = os.environ.get(\"LOCAL_RANK\", None)\n",
    "    if local_rank is None:\n",
    "        return True\n",
    "    return int(local_rank) == 0\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if is_main_process():\n",
    "    print(f\"Using device: {device}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"Number of GPUs available: {torch.cuda.device_count()}\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"Initial GPU Memory: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ede627b7-5331-4ac1-8243-5c1a534c8f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "print(\"Random seed set to 42\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64445228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading IMDB dataset...\n",
      "Dataset split complete:\n",
      "  Train: 20000 samples\n",
      "  Validation: 2500 samples\n",
      "  Test: 2500 samples\n",
      "Dataset split complete:\n",
      "  Train: 20000 samples\n",
      "  Validation: 2500 samples\n",
      "  Test: 2500 samples\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare IMDB dataset\n",
    "print(\"Loading IMDB dataset...\")\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Get full training data\n",
    "train_data = dataset[\"train\"]\n",
    "total_samples = len(train_data)\n",
    "\n",
    "# Calculate split sizes (8:1:1)\n",
    "train_size = int(0.8 * total_samples)\n",
    "val_size = int(0.1 * total_samples)\n",
    "test_size = total_samples - train_size - val_size\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset = train_data.select(range(train_size))\n",
    "val_dataset = train_data.select(range(train_size, train_size + val_size))\n",
    "test_dataset = train_data.select(range(train_size + val_size, total_samples))\n",
    "\n",
    "print(f\"Dataset split complete:\")\n",
    "print(f\"  Train: {len(train_dataset)} samples\")\n",
    "print(f\"  Validation: {len(val_dataset)} samples\")\n",
    "print(f\"  Test: {len(test_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "311d4b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "Tokenizing datasets...\n",
      "Tokenization complete!\n",
      "Tokenizing datasets...\n",
      "Tokenization complete!\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the dataset\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "print(\"Tokenizing datasets...\")\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_val = val_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "print(\"Tokenization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27246a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define compute metrics function\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"f1\": f1_score(labels, predictions, average=\"binary\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eeea7161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Trainer to track metrics per epoch with early stopping\n",
    "from transformers.trainer_callback import TrainerCallback\n",
    "\n",
    "class MetricsCallback(TrainerCallback):\n",
    "    def __init__(self, early_stop_patience=3):\n",
    "        self.epoch_times = []\n",
    "        self.epoch_accuracies = []\n",
    "        self.epoch_f1s = []\n",
    "        self.epoch_start_time = None\n",
    "        self.best_f1 = 0.0\n",
    "        self.best_f1_epoch = None\n",
    "        self.logged_epochs = set()  # Track which epochs have been logged\n",
    "\n",
    "        # Early stopping: stop when validation F1 fails to improve over best_f1 for `early_stop_patience` consecutive epochs\n",
    "        self.early_stop_patience = early_stop_patience\n",
    "        self.epochs_without_improvement = 0\n",
    "        self.early_stopped = False\n",
    "\n",
    "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
    "        self.epoch_start_time = time.time()\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        epoch_time = time.time() - self.epoch_start_time\n",
    "        self.epoch_times.append(epoch_time)\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
    "        # Called after evaluation\n",
    "        if 'eval_accuracy' in metrics and state.epoch > 0:\n",
    "            accuracy = metrics['eval_accuracy']\n",
    "            f1 = metrics.get('eval_f1', 0.0)\n",
    "            current_epoch = int(state.epoch)\n",
    "\n",
    "            # Only log data once per epoch\n",
    "            if current_epoch not in self.logged_epochs:\n",
    "                self.logged_epochs.add(current_epoch)\n",
    "                self.epoch_accuracies.append(accuracy)\n",
    "                self.epoch_f1s.append(f1)\n",
    "\n",
    "                # Update best F1 and reset counter if improved\n",
    "                if f1 > self.best_f1:\n",
    "                    self.best_f1 = f1\n",
    "                    self.best_f1_epoch = current_epoch\n",
    "                    self.epochs_without_improvement = 0\n",
    "                else:\n",
    "                    # F1 did not improve over best_f1\n",
    "                    self.epochs_without_improvement += 1\n",
    "\n",
    "                # Trigger early stopping if no improvement for patience epochs\n",
    "                if self.epochs_without_improvement >= self.early_stop_patience and not self.early_stopped:\n",
    "                    self.early_stopped = True\n",
    "                    # Request Trainer to stop training after this evaluation\n",
    "                    control.should_training_stop = True\n",
    "                    if is_main_process():\n",
    "                        print(f\"\\n[Early Stopping] No improvement over best F1 for {self.epochs_without_improvement} consecutive epochs. Stopping training at epoch {current_epoch}.\")\n",
    "                        print(f\"[Early Stopping Info] Best F1: {self.best_f1:.4f} at epoch {self.best_f1_epoch}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "372ac070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train and evaluate LoRA model with a specific rank\n",
    "def train_lora_model(rank, epochs=30, resume_from_checkpoint=False):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Training LoRA model with rank = {rank}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Reset seed for each run\n",
    "    set_seed(42)\n",
    "    \n",
    "    # Clear GPU memory\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    # Check for existing checkpoint\n",
    "    output_dir = f\"./results_imdb_lora_rank_{rank}\"\n",
    "    checkpoint_dir = None\n",
    "    if resume_from_checkpoint and os.path.exists(output_dir):\n",
    "        # Find the latest checkpoint\n",
    "        checkpoints = [d for d in os.listdir(output_dir) if d.startswith(\"checkpoint-\")]\n",
    "        if checkpoints:\n",
    "            # Sort by epoch number (checkpoint-XXXX)\n",
    "            checkpoints.sort(key=lambda x: int(x.split(\"-\")[1]))\n",
    "            checkpoint_dir = os.path.join(output_dir, checkpoints[-1])\n",
    "            print(f\"Found existing checkpoint: {checkpoint_dir}\")\n",
    "            print(f\"Resuming training from this checkpoint...\\n\")\n",
    "    \n",
    "    # Load base model\n",
    "    print(\"Loading base DistilBERT model...\")\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"distilbert-base-uncased\",\n",
    "        num_labels=2\n",
    "    )\n",
    "    \n",
    "    # Configure LoRA\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "        r=rank,\n",
    "        lora_alpha=rank * 2,  # Common practice: alpha = 2 * r\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=[\"q_lin\", \"v_lin\"],  # DistilBERT attention modules\n",
    "    )\n",
    "    \n",
    "    # Apply LoRA to the model\n",
    "    model = get_peft_model(base_model, lora_config)\n",
    "    \n",
    "    # Print model parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"\\nModel Parameters:\")\n",
    "    print(f\"  Total parameters: {total_params:,}\")\n",
    "    print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"  Trainable %: {100 * trainable_params / total_params:.2f}%\\n\")\n",
    "    \n",
    "    # Move model to device\n",
    "    model.to(device)\n",
    "\n",
    "    # Record GPU memory before training (print once using is_main_process)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        initial_memory = torch.cuda.memory_allocated() / 1024**2\n",
    "        if is_main_process():\n",
    "            print(f\"GPU Memory before training: {initial_memory:.2f} MB\")\n",
    "\n",
    "    # Training arguments - configured for multi-GPU\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        learning_rate=5e-4,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=100,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        greater_is_better=True,\n",
    "        save_total_limit=2,  # Keep 2 checkpoints to prevent data loss\n",
    "        seed=42,\n",
    "        report_to=\"none\",\n",
    "        disable_tqdm=False,\n",
    "        # Multi-GPU settings\n",
    "        local_rank=-1,  # Let Trainer handle device placement\n",
    "        dataloader_num_workers=4,  # Parallel data loading\n",
    "        dataloader_pin_memory=True,\n",
    "    )\n",
    "\n",
    "    # Create metrics callback\n",
    "    metrics_callback = MetricsCallback()\n",
    "\n",
    "    # Create trainer with callback\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_val,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[metrics_callback],\n",
    "    )\n",
    "\n",
    "    # Print starting message\n",
    "    if is_main_process():\n",
    "        if checkpoint_dir:\n",
    "            print(f\"Resuming training from checkpoint...\")\n",
    "        else:\n",
    "            print(\"Starting training from scratch...\")\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            print(f\"Using {torch.cuda.device_count()} GPUs for training\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    # Resume from checkpoint if available\n",
    "    trainer.train(resume_from_checkpoint=checkpoint_dir if checkpoint_dir else None)\n",
    "    total_training_time = time.time() - start_time\n",
    "\n",
    "    # Get peak GPU memory\n",
    "    if torch.cuda.is_available():\n",
    "        peak_memory = torch.cuda.max_memory_allocated() / 1024**2\n",
    "        print(f\"\\nPeak GPU Memory during training: {peak_memory:.2f} MB\")\n",
    "    else:\n",
    "        peak_memory = 0\n",
    "\n",
    "    # Evaluate on test set\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    test_results = trainer.evaluate(tokenized_test)\n",
    "    test_accuracy = test_results['eval_accuracy']\n",
    "    test_f1 = test_results.get('eval_f1', 0.0)\n",
    "\n",
    "    # Calculate time to convergence / time to early stop\n",
    "    # When early stopped, the training has already stopped, so use total_training_time\n",
    "    time_to_convergence = total_training_time\n",
    "\n",
    "    # Get final validation accuracy and F1\n",
    "    if len(metrics_callback.epoch_accuracies) > 0:\n",
    "        final_val_accuracy = metrics_callback.epoch_accuracies[-1]\n",
    "        final_val_f1 = metrics_callback.epoch_f1s[-1]\n",
    "    else:\n",
    "        # If epoch_accuracies is empty, evaluate manually\n",
    "        val_results = trainer.evaluate(tokenized_val)\n",
    "        final_val_accuracy = val_results['eval_accuracy']\n",
    "        final_val_f1 = val_results.get('eval_f1', 0.0)\n",
    "\n",
    "    # Compile results\n",
    "    results = {\n",
    "        \"rank\": rank,\n",
    "        \"final_test_accuracy\": test_accuracy,\n",
    "        \"final_test_f1\": test_f1,\n",
    "        \"final_val_accuracy\": final_val_accuracy,\n",
    "        \"final_val_f1\": final_val_f1,\n",
    "        \"total_parameters\": total_params,\n",
    "        \"trainable_parameters\": trainable_params,\n",
    "        \"trainable_percentage\": 100 * trainable_params / total_params,\n",
    "        \"total_training_time\": total_training_time,\n",
    "        \"average_epoch_time\": np.mean(metrics_callback.epoch_times) if len(metrics_callback.epoch_times) > 0 else 0,\n",
    "        \"epoch_times\": metrics_callback.epoch_times,\n",
    "        \"epoch_accuracies\": metrics_callback.epoch_accuracies,\n",
    "        \"epoch_f1s\": metrics_callback.epoch_f1s,\n",
    "        \"peak_gpu_memory_mb\": peak_memory,\n",
    "        \"early_stopped\": metrics_callback.early_stopped,\n",
    "        \"best_f1_epoch\": metrics_callback.best_f1_epoch,\n",
    "        \"best_val_f1\": metrics_callback.best_f1,\n",
    "    }\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Results for rank = {rank}:\")\n",
    "    print(f\"  Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"  Test F1: {test_f1:.4f}\")\n",
    "    print(f\"  Validation Accuracy: {final_val_accuracy:.4f}\")\n",
    "    print(f\"  Validation F1: {final_val_f1:.4f}\")\n",
    "    print(f\"  Total Training Time: {total_training_time:.2f}s\")\n",
    "    print(f\"  Average Epoch Time: {results['average_epoch_time']:.2f}s\")\n",
    "    print(f\"  Best F1 Epoch: {metrics_callback.best_f1_epoch}\")\n",
    "    print(f\"  Best Val F1: {metrics_callback.best_f1:.4f}\")\n",
    "    print(f\"  Early Stopped: {metrics_callback.early_stopped}\")\n",
    "    if metrics_callback.early_stopped:\n",
    "        print(f\"  Stop Info: epochs_without_improvement={metrics_callback.epochs_without_improvement}\")\n",
    "    print(f\"  Peak GPU Memory: {peak_memory:.2f} MB\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "    # Clean up\n",
    "    del model\n",
    "    del trainer\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9447e1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LoRA benchmark experiments...\n",
      "Testing ranks: [2, 4, 8, 16]\n",
      "Training epochs: 10\n",
      "Random seed: 42\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Training LoRA model with rank = 2\n",
      "================================================================================\n",
      "\n",
      "Loading base DistilBERT model...\n",
      "\n",
      "Model Parameters:\n",
      "  Total parameters: 67,584,004\n",
      "  Trainable parameters: 628,994\n",
      "  Trainable %: 0.93%\n",
      "\n",
      "GPU Memory before training: 258.90 MB\n",
      "\n",
      "Model Parameters:\n",
      "  Total parameters: 67,584,004\n",
      "  Trainable parameters: 628,994\n",
      "  Trainable %: 0.93%\n",
      "\n",
      "GPU Memory before training: 258.90 MB\n",
      "Starting training...\n",
      "Using 2 GPUs for training\n",
      "Starting training...\n",
      "Using 2 GPUs for training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LoRA benchmark experiments...\n",
      "Testing ranks: [2, 4, 8, 16]\n",
      "Training epochs: 10\n",
      "Random seed: 42\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Training LoRA model with rank = 2\n",
      "================================================================================\n",
      "\n",
      "Loading base DistilBERT model...\n",
      "\n",
      "Model Parameters:\n",
      "  Total parameters: 67,584,004\n",
      "  Trainable parameters: 628,994\n",
      "  Trainable %: 0.93%\n",
      "\n",
      "GPU Memory before training: 258.90 MB\n",
      "\n",
      "Model Parameters:\n",
      "  Total parameters: 67,584,004\n",
      "  Trainable parameters: 628,994\n",
      "  Trainable %: 0.93%\n",
      "\n",
      "GPU Memory before training: 258.90 MB\n",
      "Starting training...\n",
      "Using 2 GPUs for training\n",
      "Starting training...\n",
      "Using 2 GPUs for training\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "NCCL Error 5: invalid usage (run with NCCL_DEBUG=WARN for details)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRandom seed: 42\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m rank \u001b[38;5;129;01min\u001b[39;00m ranks_to_test:\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     results = \u001b[43mtrain_lora_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     all_results.append(results)\n\u001b[32m     14\u001b[39m     \u001b[38;5;66;03m# Save individual result\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 96\u001b[39m, in \u001b[36mtrain_lora_model\u001b[39m\u001b[34m(rank, epochs)\u001b[39m\n\u001b[32m     93\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch.cuda.device_count()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m GPUs for training\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     95\u001b[39m start_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m total_training_time = time.time() - start_time\n\u001b[32m     99\u001b[39m \u001b[38;5;66;03m# Get peak GPU memory\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/xielab/hl385/miniconda3/envs/ece685/lib/python3.12/site-packages/transformers/trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/xielab/hl385/miniconda3/envs/ece685/lib/python3.12/site-packages/transformers/trainer.py:2674\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2667\u001b[39m context = (\n\u001b[32m   2668\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2670\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2671\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2672\u001b[39m )\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2674\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/xielab/hl385/miniconda3/envs/ece685/lib/python3.12/site-packages/transformers/trainer.py:4020\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   4017\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   4019\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m4020\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4022\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   4023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4024\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4025\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   4026\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/xielab/hl385/miniconda3/envs/ece685/lib/python3.12/site-packages/transformers/trainer.py:4110\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   4108\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m] = num_items_in_batch\n\u001b[32m   4109\u001b[39m     inputs = {**inputs, **kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m4110\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4111\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   4112\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   4113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/xielab/hl385/miniconda3/envs/ece685/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/xielab/hl385/miniconda3/envs/ece685/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/xielab/hl385/miniconda3/envs/ece685/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py:193\u001b[39m, in \u001b[36mDataParallel.forward\u001b[39m\u001b[34m(self, *inputs, **kwargs)\u001b[39m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.device_ids) == \u001b[32m1\u001b[39m:\n\u001b[32m    192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.module(*inputs[\u001b[32m0\u001b[39m], **module_kwargs[\u001b[32m0\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m replicas = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreplicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    194\u001b[39m outputs = \u001b[38;5;28mself\u001b[39m.parallel_apply(replicas, inputs, module_kwargs)\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.gather(outputs, \u001b[38;5;28mself\u001b[39m.output_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/xielab/hl385/miniconda3/envs/ece685/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py:200\u001b[39m, in \u001b[36mDataParallel.replicate\u001b[39m\u001b[34m(self, module, device_ids)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreplicate\u001b[39m(\n\u001b[32m    198\u001b[39m     \u001b[38;5;28mself\u001b[39m, module: T, device_ids: Sequence[Union[\u001b[38;5;28mint\u001b[39m, torch.device]]\n\u001b[32m    199\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m[T]:\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreplicate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_grad_enabled\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/xielab/hl385/miniconda3/envs/ece685/lib/python3.12/site-packages/torch/nn/parallel/replicate.py:126\u001b[39m, in \u001b[36mreplicate\u001b[39m\u001b[34m(network, devices, detach)\u001b[39m\n\u001b[32m    124\u001b[39m params = \u001b[38;5;28mlist\u001b[39m(network.parameters())\n\u001b[32m    125\u001b[39m param_indices = {param: idx \u001b[38;5;28;01mfor\u001b[39;00m idx, param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(params)}\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m param_copies = \u001b[43m_broadcast_coalesced_reshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m buffers = \u001b[38;5;28mlist\u001b[39m(network.buffers())\n\u001b[32m    129\u001b[39m buffers_rg: \u001b[38;5;28mlist\u001b[39m[torch.Tensor] = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/xielab/hl385/miniconda3/envs/ece685/lib/python3.12/site-packages/torch/nn/parallel/replicate.py:95\u001b[39m, in \u001b[36m_broadcast_coalesced_reshape\u001b[39m\u001b[34m(tensors, devices, detach)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     93\u001b[39m     \u001b[38;5;66;03m# Use the autograd function to broadcast if not detach\u001b[39;00m\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tensors) > \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         tensor_copies = \u001b[43mBroadcast\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m     97\u001b[39m             tensor_copies[i : i + \u001b[38;5;28mlen\u001b[39m(tensors)]\n\u001b[32m     98\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(tensor_copies), \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[32m     99\u001b[39m         ]\n\u001b[32m    100\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/xielab/hl385/miniconda3/envs/ece685/lib/python3.12/site-packages/torch/autograd/function.py:576\u001b[39m, in \u001b[36mFunction.apply\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m    573\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch._C._are_functorch_transforms_active():\n\u001b[32m    574\u001b[39m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[32m    575\u001b[39m     args = _functorch.utils.unwrap_dead_wrappers(args)\n\u001b[32m--> \u001b[39m\u001b[32m576\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m    578\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[32m    579\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    580\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    581\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    582\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstaticmethod. For more details, please see \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    583\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    584\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/xielab/hl385/miniconda3/envs/ece685/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:23\u001b[39m, in \u001b[36mBroadcast.forward\u001b[39m\u001b[34m(ctx, target_gpus, *inputs)\u001b[39m\n\u001b[32m     21\u001b[39m ctx.num_inputs = \u001b[38;5;28mlen\u001b[39m(inputs)\n\u001b[32m     22\u001b[39m ctx.input_device = inputs[\u001b[32m0\u001b[39m].get_device()\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m outputs = \u001b[43mcomm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbroadcast_coalesced\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtarget_gpus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m non_differentiables = []\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, input_requires_grad \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(ctx.needs_input_grad[\u001b[32m1\u001b[39m:]):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/xielab/hl385/miniconda3/envs/ece685/lib/python3.12/site-packages/torch/nn/parallel/comm.py:66\u001b[39m, in \u001b[36mbroadcast_coalesced\u001b[39m\u001b[34m(tensors, devices, buffer_size)\u001b[39m\n\u001b[32m     64\u001b[39m devices = [_get_device_index(d) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m devices]\n\u001b[32m     65\u001b[39m tensors = [_handle_complex(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tensors]\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_broadcast_coalesced\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: NCCL Error 5: invalid usage (run with NCCL_DEBUG=WARN for details)"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRandom seed: 42\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m rank \u001b[38;5;129;01min\u001b[39;00m ranks_to_test:\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     results = \u001b[43mtrain_lora_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     all_results.append(results)\n\u001b[32m     14\u001b[39m     \u001b[38;5;66;03m# Save individual result\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 96\u001b[39m, in \u001b[36mtrain_lora_model\u001b[39m\u001b[34m(rank, epochs)\u001b[39m\n\u001b[32m     93\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch.cuda.device_count()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m GPUs for training\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     95\u001b[39m start_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m total_training_time = time.time() - start_time\n\u001b[32m     99\u001b[39m \u001b[38;5;66;03m# Get peak GPU memory\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/xielab/hl385/miniconda3/envs/ece685/lib/python3.12/site-packages/transformers/trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/xielab/hl385/miniconda3/envs/ece685/lib/python3.12/site-packages/transformers/trainer.py:2674\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2667\u001b[39m context = (\n\u001b[32m   2668\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2670\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2671\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2672\u001b[39m )\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2674\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/xielab/hl385/miniconda3/envs/ece685/lib/python3.12/site-packages/transformers/trainer.py:4020\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   4017\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   4019\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m4020\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4022\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   4023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4024\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4025\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   4026\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/xielab/hl385/miniconda3/envs/ece685/lib/python3.12/site-packages/transformers/trainer.py:4110\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   4108\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m] = num_items_in_batch\n\u001b[32m   4109\u001b[39m     inputs = {**inputs, **kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m4110\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4111\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   4112\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   4113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/xielab/hl385/miniconda3/envs/ece685/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/xielab/hl385/miniconda3/envs/ece685/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/xielab/hl385/miniconda3/envs/ece685/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py:193\u001b[39m, in \u001b[36mDataParallel.forward\u001b[39m\u001b[34m(self, *inputs, **kwargs)\u001b[39m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.device_ids) == \u001b[32m1\u001b[39m:\n\u001b[32m    192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.module(*inputs[\u001b[32m0\u001b[39m], **module_kwargs[\u001b[32m0\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m replicas = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreplicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    194\u001b[39m outputs = \u001b[38;5;28mself\u001b[39m.parallel_apply(replicas, inputs, module_kwargs)\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.gather(outputs, \u001b[38;5;28mself\u001b[39m.output_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/xielab/hl385/miniconda3/envs/ece685/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py:200\u001b[39m, in \u001b[36mDataParallel.replicate\u001b[39m\u001b[34m(self, module, device_ids)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreplicate\u001b[39m(\n\u001b[32m    198\u001b[39m     \u001b[38;5;28mself\u001b[39m, module: T, device_ids: Sequence[Union[\u001b[38;5;28mint\u001b[39m, torch.device]]\n\u001b[32m    199\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m[T]:\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreplicate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_grad_enabled\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/xielab/hl385/miniconda3/envs/ece685/lib/python3.12/site-packages/torch/nn/parallel/replicate.py:158\u001b[39m, in \u001b[36mreplicate\u001b[39m\u001b[34m(network, devices, detach)\u001b[39m\n\u001b[32m    152\u001b[39m         replica = module._replicate_for_data_parallel()\n\u001b[32m    153\u001b[39m         \u001b[38;5;66;03m# This is a temporary fix for DDP. DDP needs to access the\u001b[39;00m\n\u001b[32m    154\u001b[39m         \u001b[38;5;66;03m# replicated model parameters. It used to do so through\u001b[39;00m\n\u001b[32m    155\u001b[39m         \u001b[38;5;66;03m# `mode.parameters()`. The fix added in #33907 for DP stops the\u001b[39;00m\n\u001b[32m    156\u001b[39m         \u001b[38;5;66;03m# `parameters()` API from exposing the replicated parameters.\u001b[39;00m\n\u001b[32m    157\u001b[39m         \u001b[38;5;66;03m# Hence, we add a `_former_parameters` dict here to support DDP.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m         \u001b[43mreplica\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_former_parameters\u001b[49m = OrderedDict()\n\u001b[32m    160\u001b[39m         module_copies[j].append(replica)\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(modules):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/hpc/group/xielab/hl385/miniconda3/envs/ece685/lib/python3.12/site-packages/torch/nn/modules/module.py:2066\u001b[39m, in \u001b[36mModule.__setattr__\u001b[39m\u001b[34m(self, name, value)\u001b[39m\n\u001b[32m   2063\u001b[39m             \u001b[38;5;28mself\u001b[39m.register_buffer(name, value)\n\u001b[32m   2064\u001b[39m     \u001b[38;5;66;03m# === HACK END ===\u001b[39;00m\n\u001b[32m   2065\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2066\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__setattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Run benchmarks for all ranks\n",
    "ranks_to_test = [2, 4, 8, 16]\n",
    "all_results = []\n",
    "\n",
    "print(\"Starting LoRA benchmark experiments...\")\n",
    "print(f\"Testing ranks: {ranks_to_test}\")\n",
    "print(f\"Training epochs: 30\")\n",
    "print(f\"Random seed: 42\\n\")\n",
    "\n",
    "for rank in ranks_to_test:\n",
    "    # Set resume_from_checkpoint=True to enable automatic resume if needed\n",
    "    results = train_lora_model(rank, epochs=30)\n",
    "    all_results.append(results)\n",
    "\n",
    "    # Save individual result\n",
    "    with open(f\"imdb_lora_rank_{rank}_results.json\", \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"All experiments completed!\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ac7379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "BENCHMARK SUMMARY\n",
      "================================================================================\n",
      " Rank Test Accuracy Test F1 Val Accuracy Val F1 Total Params Trainable Params Trainable Ratio% Total Time (s) Avg Epoch Time (s)  Convergence Epoch Time to Convergence (s) Peak GPU Memory (MB)\n",
      "    2        0.8897  0.9015       0.8852 0.8954   67,584,004          628,994            0.93%        2422.85             228.30                  8                 1827.12               808.04\n",
      "    4        0.8928  0.9044       0.8889 0.8989   67,620,868          665,858            0.98%        2422.04             228.25                  8                 1825.61               806.76\n",
      "    8        0.8989  0.9096       0.8931 0.9029   67,694,596          739,586            1.09%        2405.53             226.56                 10                 2265.59               808.36\n",
      "   16        0.9050  0.9146       0.8987 0.9079   67,842,052          887,042            1.31%        2412.66             227.24                  9                 2046.06               812.39\n",
      "\n",
      " Summary saved to 'lora_benchmark_summary.csv'\n",
      " Complete results saved to 'lora_all_results.json'\n",
      "\n",
      " Summary saved to 'lora_benchmark_summary.csv'\n",
      " Complete results saved to 'lora_all_results.json'\n"
     ]
    }
   ],
   "source": [
    "# Save all results to a summary file\n",
    "import pandas as pd\n",
    "\n",
    "# Create summary DataFrame\n",
    "summary_data = []\n",
    "for result in all_results:\n",
    "    # Check if this is old data without best_val_f1\n",
    "    if 'best_val_f1' not in result:\n",
    "        print(f\"Warning: Result for rank {result['rank']} is missing 'best_val_f1'. Please re-run training.\")\n",
    "        best_val_f1_str = \"N/A\"\n",
    "        best_f1_epoch_str = \"N/A\"\n",
    "    else:\n",
    "        best_val_f1_str = f\"{result['best_val_f1']:.4f}\"\n",
    "        best_f1_epoch_str = str(result['best_f1_epoch'])\n",
    "    \n",
    "    summary_data.append({\n",
    "        \"Rank\": result[\"rank\"],\n",
    "        \"Test Acc\": f\"{result['final_test_accuracy']:.4f}\",\n",
    "        \"Test F1\": f\"{result['final_test_f1']:.4f}\",\n",
    "        \"Val Acc\": f\"{result['final_val_accuracy']:.4f}\",\n",
    "        \"Val F1\": f\"{result['final_val_f1']:.4f}\",\n",
    "        \"Best Val F1\": best_val_f1_str,\n",
    "        \"Best F1 Epoch\": best_f1_epoch_str,\n",
    "        \"Trainable Params\": f\"{result['trainable_parameters']:,}\",\n",
    "        \"Trainable %\": f\"{result['trainable_percentage']:.2f}%\",\n",
    "        \"Total Time (s)\": f\"{result['total_training_time']:.2f}\",\n",
    "        \"Avg Epoch (s)\": f\"{result['average_epoch_time']:.2f}\",\n",
    "        \"Early Stopped\": \"Yes\" if result.get('early_stopped', False) else \"No\",\n",
    "        \"Peak GPU (MB)\": f\"{result['peak_gpu_memory_mb']:.2f}\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BENCHMARK SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Save summary to CSV\n",
    "summary_df.to_csv(\"imdb_lora_benchmark_summary.csv\", index=False)\n",
    "print(\"\\n Summary saved to 'imdb_lora_benchmark_summary.csv'\")\n",
    "\n",
    "# Save complete results\n",
    "with open(\"imdb_lora_all_results.json\", \"w\") as f:\n",
    "    json.dump(all_results, f, indent=2)\n",
    "print(\" Complete results saved to 'imdb_lora_all_results.json'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece685",
   "language": "python",
   "name": "ece685"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
