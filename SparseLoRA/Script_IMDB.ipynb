{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "741d3029-9f9d-4c04-b774-cfeaac57a225",
   "metadata": {},
   "source": [
    "## Group Project LLM (IMDB)\n",
    "\n",
    "- r=2,4,8,16, epoch=10\n",
    "- seed=42\n",
    "- evaluation:\n",
    "    - accuracy, f1, precision, recall\n",
    "    - efficiency (time, trainable parameters, trainable paramters ratio, convergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36ff6988-d8be-4cad-893b-1c4e81d9d60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"default\", module=\"__main__\")\n",
    "warnings.filterwarnings(\"ignore\", module=\".*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a10aac0-477a-4871-a5cf-5ddc8633c849",
   "metadata": {},
   "source": [
    "## Base Model: DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fec7d1d-23c4-46d4-9c02-0ab56c0974e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 25000/25000 [00:00<00:00, 53605.71 examples/s]\n",
      "Generating test split: 100%|██████████| 25000/25000 [00:00<00:00, 65670.62 examples/s]\n",
      "Generating unsupervised split: 100%|██████████| 50000/50000 [00:00<00:00, 63399.59 examples/s] \n",
      "Map: 100%|██████████| 20000/20000 [00:04<00:00, 4190.15 examples/s]\n",
      "Map: 100%|██████████| 2500/2500 [00:00<00:00, 4265.27 examples/s]\n",
      "Map: 100%|██████████| 2500/2500 [00:00<00:00, 3836.64 examples/s]\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: total=66955010, trainable=66955010, ratio=100.0000%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6250/6250 20:28, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.309100</td>\n",
       "      <td>0.246099</td>\n",
       "      <td>0.894000</td>\n",
       "      <td>0.898273</td>\n",
       "      <td>0.875093</td>\n",
       "      <td>0.922713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.182600</td>\n",
       "      <td>0.255972</td>\n",
       "      <td>0.904000</td>\n",
       "      <td>0.904382</td>\n",
       "      <td>0.913849</td>\n",
       "      <td>0.895110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.108800</td>\n",
       "      <td>0.319014</td>\n",
       "      <td>0.905200</td>\n",
       "      <td>0.908388</td>\n",
       "      <td>0.890826</td>\n",
       "      <td>0.926656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.447382</td>\n",
       "      <td>0.902800</td>\n",
       "      <td>0.904743</td>\n",
       "      <td>0.899454</td>\n",
       "      <td>0.910095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.040700</td>\n",
       "      <td>0.526969</td>\n",
       "      <td>0.896800</td>\n",
       "      <td>0.901901</td>\n",
       "      <td>0.870778</td>\n",
       "      <td>0.935331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.025900</td>\n",
       "      <td>0.516620</td>\n",
       "      <td>0.905200</td>\n",
       "      <td>0.907530</td>\n",
       "      <td>0.898069</td>\n",
       "      <td>0.917192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.019000</td>\n",
       "      <td>0.575031</td>\n",
       "      <td>0.900800</td>\n",
       "      <td>0.903801</td>\n",
       "      <td>0.889313</td>\n",
       "      <td>0.918770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.012800</td>\n",
       "      <td>0.557584</td>\n",
       "      <td>0.906000</td>\n",
       "      <td>0.907298</td>\n",
       "      <td>0.907656</td>\n",
       "      <td>0.906940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.008500</td>\n",
       "      <td>0.608972</td>\n",
       "      <td>0.902400</td>\n",
       "      <td>0.902087</td>\n",
       "      <td>0.918301</td>\n",
       "      <td>0.886435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.612088</td>\n",
       "      <td>0.904800</td>\n",
       "      <td>0.905481</td>\n",
       "      <td>0.912000</td>\n",
       "      <td>0.899054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline training time: 1231.16s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='79' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [79/79 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval: {'eval_loss': 0.3191632926464081, 'eval_accuracy': 0.9068, 'eval_f1': 0.9085916045508042, 'eval_precision': 0.8832951945080092, 'eval_recall': 0.9353796445880452, 'eval_runtime': 5.713, 'eval_samples_per_second': 437.602, 'eval_steps_per_second': 13.828, 'epoch': 10.0}\n",
      "Convergence history: [{'loss': 0.3091, 'grad_norm': 4.842110633850098, 'learning_rate': 1.80032e-05, 'epoch': 1.0, 'step': 625}, {'eval_loss': 0.2460990995168686, 'eval_accuracy': 0.894, 'eval_f1': 0.8982725527831094, 'eval_precision': 0.87509349289454, 'eval_recall': 0.9227129337539433, 'eval_runtime': 5.7236, 'eval_samples_per_second': 436.788, 'eval_steps_per_second': 13.803, 'epoch': 1.0, 'step': 625}, {'loss': 0.1826, 'grad_norm': 2.859170436859131, 'learning_rate': 1.60032e-05, 'epoch': 2.0, 'step': 1250}, {'eval_loss': 0.2559722065925598, 'eval_accuracy': 0.904, 'eval_f1': 0.9043824701195219, 'eval_precision': 0.9138486312399355, 'eval_recall': 0.8951104100946372, 'eval_runtime': 5.7662, 'eval_samples_per_second': 433.56, 'eval_steps_per_second': 13.7, 'epoch': 2.0, 'step': 1250}, {'loss': 0.1088, 'grad_norm': 0.6089364290237427, 'learning_rate': 1.4003200000000002e-05, 'epoch': 3.0, 'step': 1875}, {'eval_loss': 0.31901368498802185, 'eval_accuracy': 0.9052, 'eval_f1': 0.9083880943177426, 'eval_precision': 0.8908263836239575, 'eval_recall': 0.9266561514195584, 'eval_runtime': 5.7612, 'eval_samples_per_second': 433.936, 'eval_steps_per_second': 13.712, 'epoch': 3.0, 'step': 1875}, {'loss': 0.065, 'grad_norm': 16.360395431518555, 'learning_rate': 1.2003200000000002e-05, 'epoch': 4.0, 'step': 2500}, {'eval_loss': 0.4473818838596344, 'eval_accuracy': 0.9028, 'eval_f1': 0.9047432379459036, 'eval_precision': 0.8994544037412315, 'eval_recall': 0.9100946372239748, 'eval_runtime': 5.7415, 'eval_samples_per_second': 435.425, 'eval_steps_per_second': 13.759, 'epoch': 4.0, 'step': 2500}, {'loss': 0.0407, 'grad_norm': 0.038735851645469666, 'learning_rate': 1.0003200000000001e-05, 'epoch': 5.0, 'step': 3125}, {'eval_loss': 0.5269693732261658, 'eval_accuracy': 0.8968, 'eval_f1': 0.9019011406844106, 'eval_precision': 0.8707782672540382, 'eval_recall': 0.9353312302839116, 'eval_runtime': 5.7816, 'eval_samples_per_second': 432.405, 'eval_steps_per_second': 13.664, 'epoch': 5.0, 'step': 3125}, {'loss': 0.0259, 'grad_norm': 1.2865839004516602, 'learning_rate': 8.003200000000002e-06, 'epoch': 6.0, 'step': 3750}, {'eval_loss': 0.5166202187538147, 'eval_accuracy': 0.9052, 'eval_f1': 0.907530238002341, 'eval_precision': 0.8980694980694981, 'eval_recall': 0.917192429022082, 'eval_runtime': 5.9429, 'eval_samples_per_second': 420.672, 'eval_steps_per_second': 13.293, 'epoch': 6.0, 'step': 3750}, {'loss': 0.019, 'grad_norm': 1.241469144821167, 'learning_rate': 6.0032e-06, 'epoch': 7.0, 'step': 4375}, {'eval_loss': 0.5750305652618408, 'eval_accuracy': 0.9008, 'eval_f1': 0.9038013964313422, 'eval_precision': 0.8893129770992366, 'eval_recall': 0.918769716088328, 'eval_runtime': 5.774, 'eval_samples_per_second': 432.975, 'eval_steps_per_second': 13.682, 'epoch': 7.0, 'step': 4375}, {'loss': 0.0128, 'grad_norm': 0.020683161914348602, 'learning_rate': 4.0032e-06, 'epoch': 8.0, 'step': 5000}, {'eval_loss': 0.5575835704803467, 'eval_accuracy': 0.906, 'eval_f1': 0.9072978303747534, 'eval_precision': 0.9076558800315706, 'eval_recall': 0.9069400630914827, 'eval_runtime': 6.1367, 'eval_samples_per_second': 407.386, 'eval_steps_per_second': 12.873, 'epoch': 8.0, 'step': 5000}, {'loss': 0.0085, 'grad_norm': 0.014022177085280418, 'learning_rate': 2.0032e-06, 'epoch': 9.0, 'step': 5625}, {'eval_loss': 0.6089716553688049, 'eval_accuracy': 0.9024, 'eval_f1': 0.9020866773675762, 'eval_precision': 0.9183006535947712, 'eval_recall': 0.886435331230284, 'eval_runtime': 5.7396, 'eval_samples_per_second': 435.567, 'eval_steps_per_second': 13.764, 'epoch': 9.0, 'step': 5625}, {'loss': 0.0075, 'grad_norm': 0.03711835294961929, 'learning_rate': 3.2000000000000005e-09, 'epoch': 10.0, 'step': 6250}, {'eval_loss': 0.612087607383728, 'eval_accuracy': 0.9048, 'eval_f1': 0.9054805401111994, 'eval_precision': 0.912, 'eval_recall': 0.8990536277602523, 'eval_runtime': 5.9007, 'eval_samples_per_second': 423.681, 'eval_steps_per_second': 13.388, 'epoch': 10.0, 'step': 6250}, {'train_runtime': 1230.802, 'train_samples_per_second': 162.496, 'train_steps_per_second': 5.078, 'total_flos': 1.32467398656e+16, 'train_loss': 0.07799375, 'epoch': 10.0, 'step': 6250}, {'eval_loss': 0.3191632926464081, 'eval_accuracy': 0.9068, 'eval_f1': 0.9085916045508042, 'eval_precision': 0.8832951945080092, 'eval_recall': 0.9353796445880452, 'eval_runtime': 5.713, 'eval_samples_per_second': 437.602, 'eval_steps_per_second': 13.828, 'epoch': 10.0, 'step': 6250}]\n"
     ]
    }
   ],
   "source": [
    "# ================== BASELINE DISTILBERT ================\n",
    "\n",
    "import os, time, random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    DistilBertTokenizerFast,\n",
    "    DistilBertForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    set_seed\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DATASET = \"imdb\"\n",
    "TEXT_COL = \"text\"\n",
    "LABEL_COL = \"label\"\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "LR = 2e-5\n",
    "\n",
    "\n",
    "def set_all_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    set_seed(seed)\n",
    "\n",
    "set_all_seeds(SEED)\n",
    "\n",
    "# -------- Load dataset and split (8:1:1) --------\n",
    "raw = load_dataset(DATASET)\n",
    "train_full = raw[\"train\"]\n",
    "\n",
    "train_temp = train_full.train_test_split(test_size=0.2, seed=SEED)\n",
    "train_ds = train_temp[\"train\"]\n",
    "temp = train_temp[\"test\"]\n",
    "\n",
    "val_test = temp.train_test_split(test_size=0.5, seed=SEED)\n",
    "val_ds = val_test[\"train\"]\n",
    "test_ds = val_test[\"test\"]\n",
    "\n",
    "\n",
    "# -------- Tokenization --------\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "def preprocess(x):\n",
    "    return tokenizer(x[TEXT_COL], truncation=True, max_length=256)\n",
    "\n",
    "train_ds = train_ds.map(preprocess, batched=True)\n",
    "val_ds   = val_ds.map(preprocess, batched=True)\n",
    "test_ds  = test_ds.map(preprocess, batched=True)\n",
    "\n",
    "train_ds = train_ds.rename_column(LABEL_COL, \"labels\")\n",
    "val_ds   = val_ds.rename_column(LABEL_COL, \"labels\")\n",
    "test_ds  = test_ds.rename_column(LABEL_COL, \"labels\")\n",
    "\n",
    "cols = [\"input_ids\",\"attention_mask\",\"labels\"]\n",
    "train_ds.set_format(type=\"torch\", columns=cols)\n",
    "val_ds.set_format(type=\"torch\", columns=cols)\n",
    "test_ds.set_format(type=\"torch\", columns=cols)\n",
    "\n",
    "collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# -------- Metrics --------\n",
    "acc = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "prec = evaluate.load(\"precision\")\n",
    "rec = evaluate.load(\"recall\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": acc.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "        \"f1\": f1.compute(predictions=preds, references=labels, average=\"binary\")[\"f1\"],\n",
    "        \"precision\": prec.compute(predictions=preds, references=labels, average=\"binary\")[\"precision\"],\n",
    "        \"recall\": rec.compute(predictions=preds, references=labels, average=\"binary\")[\"recall\"],\n",
    "    }\n",
    "\n",
    "# -------- Model --------\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2\n",
    ").to(DEVICE)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "ratio = trainable_params / total_params\n",
    "\n",
    "print(f\"Baseline: total={total_params}, trainable={trainable_params}, ratio={ratio:.4%}\")\n",
    "\n",
    "# -------- Train --------\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./baseline_distilbert_imdb\",\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    seed=SEED,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "trainer.train()\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Baseline training time: {end-start:.2f}s\")\n",
    "print(\"Eval:\", trainer.evaluate(test_ds))\n",
    "print(\"Convergence history:\", trainer.state.log_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1aa51c2-0a97-430e-9ef3-e8e0649673ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# -------- Final Evaluation --------\n",
    "final_metrics = trainer.evaluate(test_ds)\n",
    "\n",
    "# -------- Save metrics --------\n",
    "os.makedirs(\"./baseline_distilbert_imdb\", exist_ok=True)\n",
    "with open(\"./baseline_distilbert_imdb/final_metrics.json\", \"w\") as f:\n",
    "    json.dump(final_metrics, f, indent=4)\n",
    "\n",
    "print(\"Saved final metrics to baseline_distilbert_imdb/final_metrics.json\")\n",
    "\n",
    "# -------- Save model --------\n",
    "trainer.save_model(\"./baseline_distilbert_imdb/final_model\")\n",
    "print(\"Saved model to baseline_distilbert_imdb/final_model\")\n",
    "\n",
    "# -------- Convergence history --------\n",
    "log_history = trainer.state.log_history\n",
    "df_logs = pd.DataFrame(trainer.state.log_history)\n",
    "# Separate clean tables\n",
    "df_train = df_logs[df_logs[\"loss\"].notnull()].reset_index(drop=True)\n",
    "df_eval  = df_logs[df_logs[\"eval_loss\"].notnull()].reset_index(drop=True)\n",
    "\n",
    "df_train.to_csv(\"./baseline_distilbert_imdb/train_log.csv\", index=False)\n",
    "df_eval.to_csv(\"./baseline_distilbert_imdb/eval_log.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91335c9-b594-4f54-b5bb-e19456ea0347",
   "metadata": {},
   "source": [
    "## Sparse LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93887a12-1f4b-4390-b867-0e9c388afb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Training Sparse LoRA DistilBERT with rank = 2, epochs = 10\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rank 2] total params: 67,620,868\n",
      "[Rank 2] trainable params: 665,858\n",
      "[Rank 2] trainable params ratio (trainable / total): 0.9847%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6250/6250 18:35, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.498800</td>\n",
       "      <td>0.354335</td>\n",
       "      <td>0.853600</td>\n",
       "      <td>0.856245</td>\n",
       "      <td>0.852895</td>\n",
       "      <td>0.859621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.336800</td>\n",
       "      <td>0.326381</td>\n",
       "      <td>0.864400</td>\n",
       "      <td>0.866902</td>\n",
       "      <td>0.863174</td>\n",
       "      <td>0.870662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.314300</td>\n",
       "      <td>0.312004</td>\n",
       "      <td>0.875600</td>\n",
       "      <td>0.877318</td>\n",
       "      <td>0.877664</td>\n",
       "      <td>0.876972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.303400</td>\n",
       "      <td>0.302004</td>\n",
       "      <td>0.877600</td>\n",
       "      <td>0.879717</td>\n",
       "      <td>0.876959</td>\n",
       "      <td>0.882492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.296500</td>\n",
       "      <td>0.299173</td>\n",
       "      <td>0.873200</td>\n",
       "      <td>0.876893</td>\n",
       "      <td>0.863810</td>\n",
       "      <td>0.890379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.289800</td>\n",
       "      <td>0.292821</td>\n",
       "      <td>0.882400</td>\n",
       "      <td>0.883241</td>\n",
       "      <td>0.889600</td>\n",
       "      <td>0.876972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.288200</td>\n",
       "      <td>0.289809</td>\n",
       "      <td>0.880400</td>\n",
       "      <td>0.882237</td>\n",
       "      <td>0.881196</td>\n",
       "      <td>0.883281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.285800</td>\n",
       "      <td>0.288128</td>\n",
       "      <td>0.882400</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.880469</td>\n",
       "      <td>0.888801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.282800</td>\n",
       "      <td>0.286901</td>\n",
       "      <td>0.881200</td>\n",
       "      <td>0.882190</td>\n",
       "      <td>0.887470</td>\n",
       "      <td>0.876972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.282900</td>\n",
       "      <td>0.286499</td>\n",
       "      <td>0.882800</td>\n",
       "      <td>0.884600</td>\n",
       "      <td>0.883556</td>\n",
       "      <td>0.885647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rank 2] Training time: 1115.94 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='158' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [79/79 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rank 2] Validation metrics: {'eval_loss': 0.2881282567977905, 'eval_accuracy': 0.8824, 'eval_f1': 0.8846153846153846, 'eval_precision': 0.88046875, 'eval_recall': 0.888801261829653, 'eval_runtime': 6.6931, 'eval_samples_per_second': 373.519, 'eval_steps_per_second': 11.803, 'epoch': 10.0}\n",
      "[Rank 2] Test metrics: {'eval_loss': 0.2873707413673401, 'eval_accuracy': 0.8836, 'eval_f1': 0.8841099163679809, 'eval_precision': 0.8719560094265515, 'eval_recall': 0.8966074313408724, 'eval_runtime': 6.5961, 'eval_samples_per_second': 379.013, 'eval_steps_per_second': 11.977, 'epoch': 10.0}\n",
      "[Rank 2] LoRA sparsity (<1e-3): 13.77%\n",
      "[Rank 2] Saved model to ./sparse_lora_rank2_imdb/final_model\n",
      "[Rank 2] Saved log history to log_history.json\n",
      "\n",
      "================================================================================\n",
      "Training Sparse LoRA DistilBERT with rank = 4, epochs = 10\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rank 4] total params: 67,694,596\n",
      "[Rank 4] trainable params: 739,586\n",
      "[Rank 4] trainable params ratio (trainable / total): 1.0925%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6250/6250 18:34, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.475600</td>\n",
       "      <td>0.348947</td>\n",
       "      <td>0.858000</td>\n",
       "      <td>0.861382</td>\n",
       "      <td>0.853055</td>\n",
       "      <td>0.869874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>0.322691</td>\n",
       "      <td>0.867600</td>\n",
       "      <td>0.871156</td>\n",
       "      <td>0.860108</td>\n",
       "      <td>0.882492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.308000</td>\n",
       "      <td>0.307732</td>\n",
       "      <td>0.876800</td>\n",
       "      <td>0.878068</td>\n",
       "      <td>0.881558</td>\n",
       "      <td>0.874606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.298700</td>\n",
       "      <td>0.297457</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.881047</td>\n",
       "      <td>0.885965</td>\n",
       "      <td>0.876183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.291300</td>\n",
       "      <td>0.297204</td>\n",
       "      <td>0.878800</td>\n",
       "      <td>0.882966</td>\n",
       "      <td>0.865254</td>\n",
       "      <td>0.901420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.285200</td>\n",
       "      <td>0.289920</td>\n",
       "      <td>0.882400</td>\n",
       "      <td>0.882588</td>\n",
       "      <td>0.894013</td>\n",
       "      <td>0.871451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.283400</td>\n",
       "      <td>0.286207</td>\n",
       "      <td>0.887600</td>\n",
       "      <td>0.888801</td>\n",
       "      <td>0.891978</td>\n",
       "      <td>0.885647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.280900</td>\n",
       "      <td>0.284410</td>\n",
       "      <td>0.889200</td>\n",
       "      <td>0.890902</td>\n",
       "      <td>0.889851</td>\n",
       "      <td>0.891956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.277900</td>\n",
       "      <td>0.283639</td>\n",
       "      <td>0.887200</td>\n",
       "      <td>0.887917</td>\n",
       "      <td>0.895032</td>\n",
       "      <td>0.880915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.277500</td>\n",
       "      <td>0.283132</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.891175</td>\n",
       "      <td>0.894361</td>\n",
       "      <td>0.888013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rank 4] Training time: 1114.57 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='158' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [79/79 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rank 4] Validation metrics: {'eval_loss': 0.28313153982162476, 'eval_accuracy': 0.89, 'eval_f1': 0.891175306687772, 'eval_precision': 0.8943606036536934, 'eval_recall': 0.88801261829653, 'eval_runtime': 6.2777, 'eval_samples_per_second': 398.235, 'eval_steps_per_second': 12.584, 'epoch': 10.0}\n",
      "[Rank 4] Test metrics: {'eval_loss': 0.2854563295841217, 'eval_accuracy': 0.8904, 'eval_f1': 0.8904876099120703, 'eval_precision': 0.8813291139240507, 'eval_recall': 0.8998384491114702, 'eval_runtime': 6.4066, 'eval_samples_per_second': 390.224, 'eval_steps_per_second': 12.331, 'epoch': 10.0}\n",
      "[Rank 4] LoRA sparsity (<1e-3): 17.68%\n",
      "[Rank 4] Saved model to ./sparse_lora_rank4_imdb/final_model\n",
      "[Rank 4] Saved log history to log_history.json\n",
      "\n",
      "================================================================================\n",
      "Training Sparse LoRA DistilBERT with rank = 8, epochs = 10\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rank 8] total params: 67,842,052\n",
      "[Rank 8] trainable params: 887,042\n",
      "[Rank 8] trainable params ratio (trainable / total): 1.3075%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6250/6250 18:36, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.468400</td>\n",
       "      <td>0.353341</td>\n",
       "      <td>0.861200</td>\n",
       "      <td>0.866075</td>\n",
       "      <td>0.848073</td>\n",
       "      <td>0.884858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.331700</td>\n",
       "      <td>0.326179</td>\n",
       "      <td>0.869200</td>\n",
       "      <td>0.873501</td>\n",
       "      <td>0.857251</td>\n",
       "      <td>0.890379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.309300</td>\n",
       "      <td>0.310540</td>\n",
       "      <td>0.882400</td>\n",
       "      <td>0.883241</td>\n",
       "      <td>0.889600</td>\n",
       "      <td>0.876972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.300100</td>\n",
       "      <td>0.299509</td>\n",
       "      <td>0.885600</td>\n",
       "      <td>0.886778</td>\n",
       "      <td>0.890302</td>\n",
       "      <td>0.883281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.293300</td>\n",
       "      <td>0.302488</td>\n",
       "      <td>0.886000</td>\n",
       "      <td>0.890595</td>\n",
       "      <td>0.867614</td>\n",
       "      <td>0.914826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.286600</td>\n",
       "      <td>0.293456</td>\n",
       "      <td>0.888800</td>\n",
       "      <td>0.888532</td>\n",
       "      <td>0.903752</td>\n",
       "      <td>0.873817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.285000</td>\n",
       "      <td>0.289793</td>\n",
       "      <td>0.890400</td>\n",
       "      <td>0.891528</td>\n",
       "      <td>0.895072</td>\n",
       "      <td>0.888013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.282200</td>\n",
       "      <td>0.288134</td>\n",
       "      <td>0.892400</td>\n",
       "      <td>0.894634</td>\n",
       "      <td>0.888716</td>\n",
       "      <td>0.900631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.278300</td>\n",
       "      <td>0.287695</td>\n",
       "      <td>0.890400</td>\n",
       "      <td>0.891010</td>\n",
       "      <td>0.898876</td>\n",
       "      <td>0.883281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.278200</td>\n",
       "      <td>0.287262</td>\n",
       "      <td>0.892000</td>\n",
       "      <td>0.893196</td>\n",
       "      <td>0.896032</td>\n",
       "      <td>0.890379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rank 8] Training time: 1117.01 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='158' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [79/79 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rank 8] Validation metrics: {'eval_loss': 0.28813374042510986, 'eval_accuracy': 0.8924, 'eval_f1': 0.8946337641989816, 'eval_precision': 0.888715953307393, 'eval_recall': 0.9006309148264984, 'eval_runtime': 6.5995, 'eval_samples_per_second': 378.815, 'eval_steps_per_second': 11.971, 'epoch': 10.0}\n",
      "[Rank 8] Test metrics: {'eval_loss': 0.29389551281929016, 'eval_accuracy': 0.8872, 'eval_f1': 0.8877388535031847, 'eval_precision': 0.8751962323390895, 'eval_recall': 0.9006462035541195, 'eval_runtime': 6.407, 'eval_samples_per_second': 390.196, 'eval_steps_per_second': 12.33, 'epoch': 10.0}\n",
      "[Rank 8] LoRA sparsity (<1e-3): 23.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rank 8] Saved model to ./sparse_lora_rank8_imdb/final_model\n",
      "[Rank 8] Saved log history to log_history.json\n",
      "\n",
      "================================================================================\n",
      "Training Sparse LoRA DistilBERT with rank = 16, epochs = 10\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rank 16] total params: 68,136,964\n",
      "[Rank 16] trainable params: 1,181,954\n",
      "[Rank 16] trainable params ratio (trainable / total): 1.7347%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6250/6250 18:39, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.471200</td>\n",
       "      <td>0.363560</td>\n",
       "      <td>0.864800</td>\n",
       "      <td>0.869195</td>\n",
       "      <td>0.853343</td>\n",
       "      <td>0.885647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.343900</td>\n",
       "      <td>0.339625</td>\n",
       "      <td>0.874400</td>\n",
       "      <td>0.879509</td>\n",
       "      <td>0.856502</td>\n",
       "      <td>0.903785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.320900</td>\n",
       "      <td>0.324419</td>\n",
       "      <td>0.882400</td>\n",
       "      <td>0.882306</td>\n",
       "      <td>0.895935</td>\n",
       "      <td>0.869085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.312100</td>\n",
       "      <td>0.311804</td>\n",
       "      <td>0.885600</td>\n",
       "      <td>0.886328</td>\n",
       "      <td>0.893429</td>\n",
       "      <td>0.879338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.304500</td>\n",
       "      <td>0.320701</td>\n",
       "      <td>0.884000</td>\n",
       "      <td>0.889734</td>\n",
       "      <td>0.859031</td>\n",
       "      <td>0.922713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.297100</td>\n",
       "      <td>0.307862</td>\n",
       "      <td>0.889600</td>\n",
       "      <td>0.889246</td>\n",
       "      <td>0.905229</td>\n",
       "      <td>0.873817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.294800</td>\n",
       "      <td>0.303660</td>\n",
       "      <td>0.892400</td>\n",
       "      <td>0.893212</td>\n",
       "      <td>0.899281</td>\n",
       "      <td>0.887224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.291100</td>\n",
       "      <td>0.302278</td>\n",
       "      <td>0.893200</td>\n",
       "      <td>0.895825</td>\n",
       "      <td>0.886486</td>\n",
       "      <td>0.905363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.286700</td>\n",
       "      <td>0.301418</td>\n",
       "      <td>0.893200</td>\n",
       "      <td>0.894006</td>\n",
       "      <td>0.900080</td>\n",
       "      <td>0.888013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.286600</td>\n",
       "      <td>0.301111</td>\n",
       "      <td>0.893200</td>\n",
       "      <td>0.894257</td>\n",
       "      <td>0.898170</td>\n",
       "      <td>0.890379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rank 16] Training time: 1120.20 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='158' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [79/79 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rank 16] Validation metrics: {'eval_loss': 0.3022783696651459, 'eval_accuracy': 0.8932, 'eval_f1': 0.8958252048380804, 'eval_precision': 0.8864864864864865, 'eval_recall': 0.9053627760252366, 'eval_runtime': 6.5918, 'eval_samples_per_second': 379.262, 'eval_steps_per_second': 11.985, 'epoch': 10.0}\n",
      "[Rank 16] Test metrics: {'eval_loss': 0.3085694909095764, 'eval_accuracy': 0.8904, 'eval_f1': 0.8911834789515488, 'eval_precision': 0.8765625, 'eval_recall': 0.9063004846526656, 'eval_runtime': 6.4227, 'eval_samples_per_second': 389.247, 'eval_steps_per_second': 12.3, 'epoch': 10.0}\n",
      "[Rank 16] LoRA sparsity (<1e-3): 29.31%\n",
      "[Rank 16] Saved model to ./sparse_lora_rank16_imdb/final_model\n",
      "[Rank 16] Saved log history to log_history.json\n",
      "\n",
      "\n",
      "=== Summary over ranks (Sparse LoRA) ===\n",
      "\n",
      "Rank 2:\n",
      "  Params: 665,858 / 67,620,868 (0.98%)\n",
      "  Train time: 1115.94 s\n",
      "  Val F1:  0.8846, Acc: 0.8824\n",
      "  Test F1: 0.8841, Acc: 0.8836\n",
      "  LoRA sparsity (<1e-3): 13.77%\n",
      "\n",
      "Rank 4:\n",
      "  Params: 739,586 / 67,694,596 (1.09%)\n",
      "  Train time: 1114.57 s\n",
      "  Val F1:  0.8912, Acc: 0.8900\n",
      "  Test F1: 0.8905, Acc: 0.8904\n",
      "  LoRA sparsity (<1e-3): 17.68%\n",
      "\n",
      "Rank 8:\n",
      "  Params: 887,042 / 67,842,052 (1.31%)\n",
      "  Train time: 1117.01 s\n",
      "  Val F1:  0.8946, Acc: 0.8924\n",
      "  Test F1: 0.8877, Acc: 0.8872\n",
      "  LoRA sparsity (<1e-3): 23.14%\n",
      "\n",
      "Rank 16:\n",
      "  Params: 1,181,954 / 68,136,964 (1.73%)\n",
      "  Train time: 1120.20 s\n",
      "  Val F1:  0.8958, Acc: 0.8932\n",
      "  Test F1: 0.8912, Acc: 0.8904\n",
      "  LoRA sparsity (<1e-3): 29.31%\n"
     ]
    }
   ],
   "source": [
    "# ================== SPARSE LoRA MODEL =================\n",
    "\n",
    "from typing import Dict, Any, List, Optional\n",
    "import math\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# -------- Sparse LoRA config --------\n",
    "RANKS: List[int] = [2, 4, 8, 16]\n",
    "L1_LAMBDA = 1e-5   # sparsity strength for LoRA weights\n",
    "\n",
    "\n",
    "def count_trainable_params(model: torch.nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def count_total_params(model: torch.nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "\n",
    "def compute_lora_sparsity(model: torch.nn.Module, threshold: float = 1e-3) -> float:\n",
    "    \"\"\"\n",
    "    Approximate sparsity: fraction of LoRA parameters with |w| < threshold.\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    near_zero = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"lora_\" in name and param.requires_grad:\n",
    "            data = param.detach().abs()\n",
    "            total += data.numel()\n",
    "            near_zero += (data < threshold).sum().item()\n",
    "    return near_zero / total if total > 0 else math.nan\n",
    "\n",
    "\n",
    "class SparseLoraTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Trainer with L1 penalty only on LoRA parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self, l1_lambda: float = 0.0, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.l1_lambda = l1_lambda\n",
    "\n",
    "    def compute_loss(\n",
    "        self,\n",
    "        model,\n",
    "        inputs,\n",
    "        return_outputs: bool = False,\n",
    "        num_items_in_batch: Optional[int] = None,\n",
    "    ):\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        if self.l1_lambda > 0:\n",
    "            l1_reg = 0.0\n",
    "            for name, param in model.named_parameters():\n",
    "                if \"lora_\" in name and param.requires_grad:\n",
    "                    l1_reg = l1_reg + param.abs().sum()\n",
    "            loss = loss + self.l1_lambda * l1_reg\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "results_per_rank: List[Dict[str, Any]] = []\n",
    "\n",
    "for r in RANKS:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"Training Sparse LoRA DistilBERT with rank = {r}, epochs = {NUM_EPOCHS}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    set_all_seeds(SEED)\n",
    "\n",
    "    # Base DistilBERT for this rank\n",
    "    base_model = DistilBertForSequenceClassification.from_pretrained(\n",
    "        \"distilbert-base-uncased\",\n",
    "        num_labels=2,\n",
    "    )\n",
    "\n",
    "    # LoRA config: attention projections in DistilBERT\n",
    "    lora_config = LoraConfig(\n",
    "        r=r,\n",
    "        lora_alpha=2 * r,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=\"SEQ_CLS\",    # sequence classification\n",
    "        target_modules=[\"q_lin\", \"k_lin\", \"v_lin\", \"out_lin\"],\n",
    "    )\n",
    "\n",
    "    lora_model = get_peft_model(base_model, lora_config)\n",
    "    lora_model.to(DEVICE)\n",
    "\n",
    "    total_params = count_total_params(lora_model)\n",
    "    trainable_params = count_trainable_params(lora_model)\n",
    "    param_ratio = trainable_params / total_params\n",
    "\n",
    "    print(f\"[Rank {r}] total params: {total_params:,}\")\n",
    "    print(f\"[Rank {r}] trainable params: {trainable_params:,}\")\n",
    "    print(f\"[Rank {r}] trainable params ratio (trainable / total): {param_ratio:.4%}\")\n",
    "\n",
    "    output_dir = f\"./sparse_lora_rank{r}_imdb\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    training_args_lora = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        learning_rate=LR,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        greater_is_better=True,\n",
    "        seed=SEED,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    trainer = SparseLoraTrainer(\n",
    "        l1_lambda=L1_LAMBDA,\n",
    "        model=lora_model,\n",
    "        args=training_args_lora,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "    trainer.train()\n",
    "    end_time = time.time()\n",
    "    train_time = end_time - start_time\n",
    "    print(f\"[Rank {r}] Training time: {train_time:.2f} seconds\")\n",
    "\n",
    "    # --- final evals ---\n",
    "    val_metrics = trainer.evaluate(eval_dataset=val_ds)\n",
    "    test_metrics = trainer.evaluate(eval_dataset=test_ds)\n",
    "    lora_sparsity = compute_lora_sparsity(lora_model, threshold=1e-3)\n",
    "\n",
    "    print(f\"[Rank {r}] Validation metrics: {val_metrics}\")\n",
    "    print(f\"[Rank {r}] Test metrics: {test_metrics}\")\n",
    "    print(f\"[Rank {r}] LoRA sparsity (<1e-3): {lora_sparsity:.2%}\")\n",
    "\n",
    "    # ==========================\n",
    "    # SAVE METRICS / MODEL / LOG\n",
    "    # ==========================\n",
    "    # 1) save metrics\n",
    "    metrics_payload = {\n",
    "        \"rank\": r,\n",
    "        \"train_time_sec\": train_time,\n",
    "        \"total_params\": int(total_params),\n",
    "        \"trainable_params\": int(trainable_params),\n",
    "        \"param_ratio\": float(param_ratio),\n",
    "        \"lora_sparsity_<1e-3\": float(lora_sparsity),\n",
    "        \"val_metrics\": val_metrics,\n",
    "        \"test_metrics\": test_metrics,\n",
    "    }\n",
    "    with open(os.path.join(output_dir, \"final_metrics.json\"), \"w\") as f:\n",
    "        json.dump(metrics_payload, f, indent=4)\n",
    "\n",
    "    # 2) save final model (best checkpoint)\n",
    "    final_model_dir = os.path.join(output_dir, \"final_model\")\n",
    "    trainer.save_model(final_model_dir)  # saves model + config\n",
    "    tokenizer.save_pretrained(final_model_dir)  # save tokenizer too\n",
    "    print(f\"[Rank {r}] Saved model to {final_model_dir}\")\n",
    "\n",
    "    # 3) save convergence history\n",
    "    log_history = trainer.state.log_history\n",
    "    with open(os.path.join(output_dir, \"log_history.json\"), \"w\") as f:\n",
    "        json.dump(log_history, f, indent=4)\n",
    "    print(f\"[Rank {r}] Saved log history to log_history.json\")\n",
    "\n",
    "    # --- store in-memory summary for printing ---\n",
    "    results_per_rank.append(\n",
    "        {\n",
    "            \"rank\": r,\n",
    "            \"total_params\": total_params,\n",
    "            \"trainable_params\": trainable_params,\n",
    "            \"param_ratio\": param_ratio,\n",
    "            \"train_time_sec\": train_time,\n",
    "            \"val_metrics\": val_metrics,\n",
    "            \"test_metrics\": test_metrics,\n",
    "            \"lora_sparsity(<1e-3)\": lora_sparsity,\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\\n\\n=== Summary over ranks (Sparse LoRA) ===\")\n",
    "for res in results_per_rank:\n",
    "    r = res[\"rank\"]\n",
    "    print(f\"\\nRank {r}:\")\n",
    "    print(f\"  Params: {res['trainable_params']:,} / {res['total_params']:,} \"\n",
    "          f\"({res['param_ratio']:.2%})\")\n",
    "    print(f\"  Train time: {res['train_time_sec']:.2f} s\")\n",
    "    print(f\"  Val F1:  {res['val_metrics'].get('eval_f1', float('nan')):.4f}, \"\n",
    "          f\"Acc: {res['val_metrics'].get('eval_accuracy', float('nan')):.4f}\")\n",
    "    print(f\"  Test F1: {res['test_metrics'].get('eval_f1', float('nan')):.4f}, \"\n",
    "          f\"Acc: {res['test_metrics'].get('eval_accuracy', float('nan')):.4f}\")\n",
    "    print(f\"  LoRA sparsity (<1e-3): {res['lora_sparsity(<1e-3)']:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1a5cd9-2106-44f4-a488-c855e88d081e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d18ca5-97fc-4790-ad75-47b3a0a56a85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-conda-env-kernel",
   "language": "python",
   "name": "my-conda-env-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
